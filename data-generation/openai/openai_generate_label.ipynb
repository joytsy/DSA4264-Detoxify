{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: sk-afklp7QTf3QSs-7f7ufW8E-9c-YjY64thYzjuHk5ThT3BlbkFJCbUZuS8Dq8QlRMKRBO_uHsho_ujq9OEZ6OAnzBIb8A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"Loaded API Key: {api_key}\")\n",
    "\n",
    "# Check if the API key was loaded correctly\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not found in environment variables!\")\n",
    "\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai_requests_json(input_file, chunk_size):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a multiclass text classifier. Comments should be taken as they are, without inferring tone or context. \n",
    "    Step 1: Group Sensitivity Determine if the text refers to a group defined by any of the following characteristics: \n",
    "    Nationality, Race, Ethnicity, Religion, Gender, Sexual orientation, Disability, Skin color, Age, Generational group, \n",
    "    Socio-economic status, Immigration status. Sensitive Group: If the text explicitly mentions a group belonging to any of these characteristics, \n",
    "    classify it as Sensitive Group: Yes. Non-Sensitive Group: If the text does not mention any of these characteristics, classify it as Sensitive Group: \n",
    "    No. Step 2: Language Intensity Classification For Sensitive Groups (Hate) Hate 1: Statements containing bias or generalizations without exclusion or incitement. \n",
    "    Hate 2: Statements that add exclusion or denial of rights to Hate 1. This includes explicit or implicit denial of access, rights, opportunities, \n",
    "    or services based on the group's characteristics. Hate 3: Statements that incite harm or make threats. For Non-Sensitive Groups (Toxic) \n",
    "    Toxic 1: Mild complaints or disrespect without direct insults or threats. Toxic 2: Clear insults, sarcasm, or mocking. Toxic 3: Harassment or encouragement of harm. \n",
    "    For Any Groups No Hate/Toxic: Neutral or constructive comments, including mild criticism or playful sarcasm. Output Format Sensitive Group: Yes/No Classification: (e.g., \"No Hate/Toxic\", \"Hate 1\", etc.) Examples: \n",
    "    Example 1: \"Expats in Singapore always expect preferential treatment.\" Sensitive Group: Yes Classification: Hate 1 Example 2: \"I don’t let Indians in \n",
    "    because they are smelly.\" Sensitive Group: Yes Classification: Hate 2 Example 3: \"Gen Zs don’t belong here. I’ll kill them if they stay.\" Sensitive Group: Yes Classification: \n",
    "    Hate 3 Classify this comment: \n",
    "    \"\"\"\n",
    "\n",
    "    # Read input CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    sensitive_groups = []\n",
    "    classifications = []\n",
    "\n",
    "    # Process each comment individually\n",
    "    for index, row in df.iterrows():\n",
    "        comment = row['text']\n",
    "        \n",
    "        requests = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": comment}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Sending request for the current comment\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",  # Specify your model here\n",
    "                messages=requests,\n",
    "                max_tokens=100,\n",
    "                n=1,  # Generate one response for the current comment\n",
    "            )\n",
    "\n",
    "            # Process response\n",
    "            choice = response.choices[0]\n",
    "            result = choice.message.content.strip()\n",
    "            # Assuming the output is in the format: \"Sensitive Group: Yes Classification: Hate 1\"\n",
    "            sensitive_group, classification = result.split(\"Classification:\")\n",
    "            sensitive_groups.append(sensitive_group.replace(\"Sensitive Group:\", \"\").strip())\n",
    "            classifications.append(classification.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            # Append None if there's an error for the current comment\n",
    "            sensitive_groups.append(None)\n",
    "            classifications.append(None)\n",
    "\n",
    "        # Save results after every `chunk_size` comments\n",
    "        if (index + 1) % chunk_size == 0:\n",
    "            # Append results to the original DataFrame\n",
    "            start_index = index - chunk_size + 1\n",
    "            df.loc[start_index:index, 'Sensitive Group'] = sensitive_groups[start_index:index + 1]\n",
    "            df.loc[start_index:index, 'Classification'] = classifications[start_index:index + 1]\n",
    "\n",
    "            # Save the updated DataFrame back to the same CSV file\n",
    "            df.to_csv(input_file, index=False)\n",
    "\n",
    "    # Save any remaining results if they exist after the loop\n",
    "    if len(sensitive_groups) > len(df):\n",
    "        # Append any remaining results\n",
    "        df['Sensitive Group'] = sensitive_groups\n",
    "        df['Classification'] = classifications\n",
    "        df.to_csv(input_file, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this\n",
    "input_file= \"C:\\\\Users\\\\Yi Jing\\\\Documents\\\\Y4S1\\\\dsa4264\\\\DSA4264-Detoxify\\\\predicted_labels_output_2023 - Copy.csv\"\n",
    "generate_openai_requests_json(input_file,chunk_size=100) # change chunk size if needed, happy labelling! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4264-Detoxify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
