{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define informal templates for Hate 3 sentences (15 Singaporean-English mix)\n",
    "hate_3_templates = [\n",
    "    \"If {sensitive_groups} dont {action}, theyre gonna {threat}.\",\n",
    "    \"Only way to deal with {sensitive_groups} not {action} is to {threat}.\",\n",
    "    \"{sensitive_groups} not {action}? Yeah, theyll {threat} real soon.\",\n",
    "    \"If {sensitive_groups} keep {action}, Ill {threat}.\",\n",
    "    \"Im just waiting for {sensitive_groups} to {action}, or else theyll {threat}.\",\n",
    "    \"When {sensitive_groups} {action}, theyll regret it 'cause Ill {threat}.\",\n",
    "    \"Watch {sensitive_groups} {action} and then {threat} happens.\",\n",
    "    \"Gonna {threat} if {sensitive_groups} dont {action}.\",\n",
    "    \"{sensitive_groups} better {action}, or its {threat} for them.\",\n",
    "    \"{sensitive_groups} must {action}, or they will definitely {threat}.\",\n",
    "    \"If {sensitive_groups} dont {action}, theyll be in for {threat}.\",\n",
    "    \"{sensitive_groups} better {action}, or theyll face {threat} soon.\",\n",
    "    \"No chance for {sensitive_groups} to avoid {threat} if they dont {action}.\",\n",
    "    \"{sensitive_groups} will {threat} if they dont start {action}.\",\n",
    "    \"As soon as {sensitive_groups} stop {action}, theyll kena {threat}.\",\n",
    "]\n",
    "\n",
    "# List of 10 sensitive groups\n",
    "sensitive_groups = [\n",
    "    \"Burmese\",\n",
    "    \"Malays\",\n",
    "    \"Indians\",\n",
    "    \"Christian\",\n",
    "    \"Jews\",\n",
    "    \"Lesbian\",\n",
    "    \"Gay\",\n",
    "    \"Disabled\",\n",
    "    \"Poor\",\n",
    "    \"Immigrant\",\n",
    "]\n",
    "\n",
    "# Define varied actions (10 Singaporean-English mix)\n",
    "actions = [\n",
    "    \"pack up and leave\",\n",
    "    \"stop causing trouble\",\n",
    "    \"get out of this country\",\n",
    "    \"stop ruining everything\",\n",
    "    \"stay away from us\",\n",
    "    \"stop being so kaypoh\",\n",
    "    \"stay out of our lives\",\n",
    "    \"go back to wherever they came from\",\n",
    "    \"get out of our way\",\n",
    "    \"stop taking our jobs\",\n",
    "]\n",
    "\n",
    "# Define more varied informal threats (10 threats)\n",
    "threats = [\n",
    "    \"regret messing with us\",\n",
    "    \"be gone forever\",\n",
    "    \"make their lives miserable\",\n",
    "    \"regret sticking around\",\n",
    "    \"wish they never showed up\",\n",
    "    \"lose everything they have\",\n",
    "    \"end up in shit\",\n",
    "    \"burn their houses\",\n",
    "    \"get completely destroyed\",\n",
    "    \"break their bones\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Cartesian product of all combinations\n",
    "\n",
    "\n",
    "def generate_deterministic_hate_3_comments():\n",
    "    # Create combinations using the product of templates, groups, actions, and threats\n",
    "    combinations = itertools.product(\n",
    "        hate_3_templates, sensitive_groups, actions, threats\n",
    "    )\n",
    "\n",
    "    # Format the combinations with the correct placeholders for 'sensitive_groups', 'action', and 'threat'\n",
    "    comments = [\n",
    "        template.format(sensitive_groups=sensitive_group, action=action, threat=threat)\n",
    "        for template, sensitive_group, action, threat in combinations\n",
    "    ]\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique ID (e.g., comment id)\n",
    "\n",
    "\n",
    "def generate_unique_comment_id():\n",
    "    return \"\".join(random.choices(\"abcdefghijklmnopqrstuvwxyz0123456789\", k=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique link ID (e.g., \"t3_\" followed by 6 characters)\n",
    "\n",
    "\n",
    "def generate_unique_link_id():\n",
    "    return \"t3_\" + \"\".join(random.choices(\"abcdefghijklmnopqrstuvwxyz0123456789\", k=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random timestamp (date and time) in 2023\n",
    "\n",
    "\n",
    "def generate_random_timestamp(year=2023):\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "    random_date = start_date + timedelta(\n",
    "        days=random.randint(0, (end_date - start_date).days)\n",
    "    )\n",
    "\n",
    "    # Generate random hours, minutes, and seconds\n",
    "    random_time = timedelta(\n",
    "        hours=random.randint(0, 23),\n",
    "        minutes=random.randint(0, 59),\n",
    "        seconds=random.randint(0, 59),\n",
    "    )\n",
    "\n",
    "    # Combine the random date with the random time\n",
    "    full_random_datetime = random_date + random_time\n",
    "\n",
    "    # Return in the desired format\n",
    "    return full_random_datetime.strftime(\"%m/%d/%Y %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random 6-letter username (alphabets only)\n",
    "\n",
    "\n",
    "def generate_username():\n",
    "    return \"\".join(random.choices(\"abcdefghijklmnopqrstuvwxyz\", k=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5000 deterministic comments\n",
    "hate_3_data = generate_deterministic_hate_3_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file columns\n",
    "csv_columns = [\n",
    "    \"text\",\n",
    "    \"timestamp\",\n",
    "    \"username\",\n",
    "    \"link\",\n",
    "    \"link_id\",\n",
    "    \"parent_id\",\n",
    "    \"id\",\n",
    "    \"subreddit_id\",\n",
    "    \"moderation\",\n",
    "    \"year\",\n",
    "    \"word_count\",\n",
    "    \"Sensitive Group\",\n",
    "    \"Classification\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant values\n",
    "subreddit_id = \"t5_2qh8c\"\n",
    "moderation = \"{'controversiality': 0, 'collapsed_reason_code': None, 'collapsed': False, 'collapsed_reason': None}\"\n",
    "year = \"2023\"\n",
    "classification = \"Hate 3\"  # Change from gold_label to Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare rows for CSV\n",
    "csv_data = []\n",
    "for i in range(15000):\n",
    "    text = hate_3_data[i]\n",
    "    timestamp = generate_random_timestamp()\n",
    "    username = generate_username()\n",
    "    link_id = generate_unique_link_id()\n",
    "    parent_id = link_id  # Same within each row\n",
    "    comment_id = generate_unique_comment_id()\n",
    "    link = f\"/r/singapore/comments/13c3mt8/Strict_appearance_and_grooming_standards_for_Singapore_Airlines_cabin_crew/{comment_id}/\"\n",
    "\n",
    "    # Create a row for the CSV with columns matching balanced_data\n",
    "    row = {\n",
    "        \"text\": text,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"username\": username,\n",
    "        \"link\": link,\n",
    "        \"link_id\": link_id,\n",
    "        \"parent_id\": parent_id,\n",
    "        \"id\": comment_id,\n",
    "        \"subreddit_id\": subreddit_id,\n",
    "        \"moderation\": moderation,\n",
    "        \"year\": year,\n",
    "        \"word_count\": len(text.split()),  # Assuming you want to keep the word count\n",
    "        \"Sensitive Group\": None,  # Or provide a value if needed\n",
    "        \"Classification\": classification,  # Use the appropriate classification\n",
    "    }\n",
    "\n",
    "    csv_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'hate_3_comments.csv' with 156000 rows generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# After generating all the rows in csv_data, shuffle them\n",
    "random.shuffle(csv_data)\n",
    "\n",
    "# Randomly sample 9861 rows\n",
    "sampled_data = random.sample(csv_data, 14779)\n",
    "\n",
    "# Define the column headers for the CSV\n",
    "csv_columns = csv_data[0].keys()\n",
    "\n",
    "# Writing the shuffled and sampled data to a CSV file\n",
    "csv_file = \"hate_3_comments.csv\"\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sampled_data)\n",
    "\n",
    "print(f\"CSV file '{csv_file}' with 156000 rows generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
