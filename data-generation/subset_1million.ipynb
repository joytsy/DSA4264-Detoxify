{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4381725, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    r\"C:\\Users\\user\\OneDrive - National University of Singapore\\Desktop\\Y4S1\\DSA4264\\DSA4264-Detoxify\\kedro-data\\data\\03_primary\\clean_gt_texts_data.csv\\2024-10-23T11.19.48.213Z\\clean_gt_texts_data.csv\"\n",
    ")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'timestamp', 'username', 'link', 'link_id', 'parent_id', 'id', 'subreddit_id', 'moderation', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word counts: 1641\n",
      "word_count\n",
      "1.0      126673\n",
      "2.0      130028\n",
      "3.0      152811\n",
      "4.0      166991\n",
      "5.0      173137\n",
      "          ...  \n",
      "96.0       4571\n",
      "97.0       4332\n",
      "98.0       4195\n",
      "99.0       4029\n",
      "100.0      4096\n",
      "Name: count, Length: 100, dtype: int64\n",
      "word_count\n",
      "1589.0    2\n",
      "1590.0    1\n",
      "1591.0    2\n",
      "1592.0    2\n",
      "1593.0    1\n",
      "         ..\n",
      "1840.0    1\n",
      "1842.0    1\n",
      "1864.0    1\n",
      "1868.0    1\n",
      "1872.0    1\n",
      "Name: count, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the word count for each text entry\n",
    "data[\"word_count\"] = data[\"text\"].str.split(\" \").str.len()\n",
    "\n",
    "# Count the occurrences of each unique word count\n",
    "word_count_counts = data[\"word_count\"].value_counts().sort_index()\n",
    "\n",
    "# Display the number of unique word counts\n",
    "num_unique_word_counts = word_count_counts.count()\n",
    "print(f\"Number of unique word counts: {num_unique_word_counts}\")\n",
    "\n",
    "# Display more results\n",
    "word_count_counts_display = word_count_counts[word_count_counts > 0]\n",
    "print(\n",
    "    word_count_counts_display.head(100)\n",
    ")  # Display the first 100 word counts for a quick overview\n",
    "print(\n",
    "    word_count_counts_display.tail(100)\n",
    ")  # Display the last 100 word counts for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean word count: 32.32\n",
      "Median word count: 16.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and median word count\n",
    "mean_word_count = data[\"word_count\"].mean()\n",
    "median_word_count = data[\"word_count\"].median()\n",
    "\n",
    "print(f\"Mean word count: {mean_word_count:.2f}\")\n",
    "print(f\"Median word count: {median_word_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with more than 5 words and 50 words or less: 3117245\n"
     ]
    }
   ],
   "source": [
    "# Count entries with more than 5 words and 80 words or less\n",
    "count_entries = data[(data[\"word_count\"] >= 5) & (data[\"word_count\"] <= 50)].shape[0]\n",
    "\n",
    "print(f\"Number of entries with more than 5 words and 50 words or less: {count_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           text\n",
      "1041425               You broke up recently too\n",
      "2341273              How to reschedule Help pls\n",
      "2075990                         Why M and not P\n",
      "2413377                 Knn 14 minimum is crazy\n",
      "642730                 I smell a lawsuit coming\n",
      "1652877             Yeah that's the plus point!\n",
      "3395226          Thank you for the explanation.\n",
      "3030423                Or cher in the classroom\n",
      "2667886             Eating in moderation, is ok\n",
      "3866161  aka dishonorable discharge. SAY SORRY!\n"
     ]
    }
   ],
   "source": [
    "# one_char_texts = data[data['text'].str.len() == 5]\n",
    "five_words = data[data[\"word_count\"] == 5]\n",
    "# Get 10 examples\n",
    "examples = (\n",
    "    five_words.sample(n=10, random_state=62) if len(five_words) >= 10 else five_words\n",
    ")\n",
    "\n",
    "# Display the examples\n",
    "print(examples[[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampling complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "# Filter for entries with at least 5 words and less than or equal to 80 words\n",
    "filtered_data = data[(data[\"word_count\"] >= 5) & (data[\"word_count\"] <= 50)]\n",
    "\n",
    "# Shuffle the filtered dataset\n",
    "filtered_data = filtered_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Take a subsample of 1 million rows\n",
    "subset_size = min(1000000, len(filtered_data))\n",
    "subset = filtered_data.iloc[:subset_size]\n",
    "\n",
    "# Keep only the \"text\" column\n",
    "subset = subset[[\"text\"]]\n",
    "\n",
    "# Split the subset into 5 different CSV files\n",
    "num_files = 5\n",
    "rows_per_file = len(subset) // num_files\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * rows_per_file\n",
    "    end_idx = (\n",
    "        (i + 1) * rows_per_file if i < num_files - 1 else len(subset)\n",
    "    )  # Handle the last file\n",
    "    subset.iloc[start_idx:end_idx].to_csv(\n",
    "        f\"1million_subset_part_{i + 1}.csv\", index=False\n",
    "    )\n",
    "\n",
    "print(\"Subsampling complete. Files saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
