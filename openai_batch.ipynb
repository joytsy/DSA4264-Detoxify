{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai_batch_requests_json(\n",
    "    csv_file, output_file, start_index, end_index, max_requests\n",
    "):\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert annotator. Your task is to classify the following text into one of the following categories:\n",
    "    1. No Hate/Toxic: The comment contains no hate or toxic speech.\n",
    "    2. Hate 1: Bias and prejudice (e.g., attitudes or beliefs about a group).\n",
    "    3. Hate 2: Discrimination (expressing dislike or exclusion but without a call to action).\n",
    "    4. Hate 3: Threats and violence (contains direct threats, incitement to violence, or serious harm).\n",
    "    5. Toxic 1: Baseless complaints (complaints or frustrations without direct harm).\n",
    "    6. Toxic 2: Insults or sarcasm directed at someone (humiliation or belittling without inciting violence).\n",
    "    7. Toxic 3: Insults with high level of hostility, aggressive, harassment, or threats of harm (incitement to self-harm or targeted harassment).\n",
    "    Please classify each comment based on the most severe form of speech present, either hate or toxic speech (but not both). The model should internally reason through the following process but only output the final classification (e.g., Toxic 2, Hate 1, etc.).\n",
    "    Step-by-Step Classification Process:\n",
    "    1. Identify Target: Determine whether the text targets a specific group or an individual (group = hate, individual = toxic).\n",
    "    2. Assess Intensity: Evaluate the severity of the statement. Does it express an opinion, incite discrimination, or make threats?\n",
    "    3. Apply Hate or Toxic Levels: Assign a label based on the most severe behavior present in the text (choose either hate or toxic, not both).\n",
    "    4. Final Classification: Choose the highest relevant category based on the content.\n",
    "    Examples:\n",
    "    Example 1:\n",
    "    Text: \"Foreigners are taking all our jobs.\"\n",
    "    Hate 1\n",
    "    Example 2:\n",
    "    Text: \"Youâ€™re an idiot, why do you even try?\"\n",
    "    Toxic 2\n",
    "    Example 3:\n",
    "    Text: \"I refuse to serve Black people in my restaurant.\"\n",
    "    Hate 2\n",
    "    Example 4:\n",
    "    Text: \"Go end your life, no one cares about you.\"\n",
    "    Toxic 3\n",
    "    Classify the text given by the user, output only the final classification. \n",
    "    \"\"\"\n",
    "\n",
    "    request_count = 0\n",
    "\n",
    "    with open(csv_file, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        with open(output_file, mode=\"w\", encoding=\"utf-8\") as output:\n",
    "            for i, row in enumerate(csv_reader):\n",
    "                # Check if the row falls within the start and end index range\n",
    "                if i < start_index:\n",
    "                    continue\n",
    "                if i >= end_index or request_count >= max_requests:\n",
    "                    break\n",
    "\n",
    "                custom_id = f\"request-{i + 1}\"\n",
    "                user_prompt = row[\"text\"]\n",
    "                request = {\n",
    "                    \"custom_id\": custom_id,\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        \"model\": \"gpt-4o-mini\",\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt},\n",
    "                        ],\n",
    "                        \"max_tokens\": 10,\n",
    "                    },\n",
    "                }\n",
    "                # Write each request as a separate JSON object on a new line\n",
    "                output.write(json.dumps(request) + \"\\n\")\n",
    "\n",
    "                request_count += 1  # Increment the request counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "batch_number = 7\n",
    "\n",
    "generate_openai_batch_requests_json(\n",
    "    csv_file=\"clean_concatenated_texts_data.csv\",\n",
    "    output_file=f\"batch_requests_{batch_number}.json1\",\n",
    "    start_index=6000,  # Specify start index\n",
    "    end_index=36000,  # Specify end index\n",
    "    max_requests=30000,  # Specify max number of requests to generate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Your Batch Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(\n",
    "        f\"C:/Users/richm/OneDrive/Desktop/DSA4264/DSA4264-Detoxify/batch_requests_{batch_number}.json1\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    purpose=\"batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_67063b0fda0c819087744646029c276e', completion_window='24h', created_at=1728461583, endpoint='/v1/chat/completions', input_file_id='file-4XZOyxf2RZfKTzSEpSIWktEg', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1728547983, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'classification of toxic/hate comments batch_7': 'classification of toxic/hate comments batch_7'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "description = f\"classification of toxic/hate comments batch_{batch_number}\"\n",
    "\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={description: description},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Status of a Batch\n",
    "https://platform.openai.com/batches/batch_670627cc819c819090732c245d2543c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67063b0fda0c819087744646029c276e', completion_window='24h', created_at=1728461583, endpoint='/v1/chat/completions', input_file_id='file-4XZOyxf2RZfKTzSEpSIWktEg', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1728547983, failed_at=None, finalizing_at=None, in_progress_at=1728461607, metadata={'classification of toxic/hate comments batch_7': 'classification of toxic/hate comments batch_7'}, output_file_id=None, request_counts=BatchRequestCounts(completed=9030, failed=0, total=30000))\n"
     ]
    }
   ],
   "source": [
    "# print(client.batches.retrieve(\"batch_6706205e87808190b0658d09d60b64ae\"))\n",
    "# print(client.batches.retrieve(\"batch_67062689346081909f6d648882586a9f\"))\n",
    "# print(client.batches.retrieve(\"batch_67062782c47c8190ad7047745d92dc88\"))\n",
    "# print(client.batches.retrieve(\"batch_6706352d233481908494cc98ad3722d9\"))\n",
    "# print(client.batches.retrieve(\"batch_6706354d17c8819090d439ba3c5e794d\"))\n",
    "# print(client.batches.retrieve(\"batch_670635699a288190bdaee914d22a71c8\"))\n",
    "print(\n",
    "    client.batches.retrieve(\"batch_67063b0fda0c819087744646029c276e\")\n",
    ")  # not processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving the Results of a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 3', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Hate 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 3', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'Toxic 3', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Hate 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Hate 3', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 3', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Hate 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 3', 'Toxic 2', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Hate 3', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Hate 3', 'No Hate/Toxic', 'Hate 1', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 3', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Hate 1', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Hate 2', 'Toxic 2', 'Toxic 2', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Hate 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Hate 1', 'No Hate/Toxic', 'Hate 1', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'No Hate/Toxic', 'Hate 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'No Hate/Toxic', 'Hate 1', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Hate 1', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 1', 'No Hate/Toxic', 'Toxic 1', 'Toxic 2', 'No Hate/Toxic', 'No Hate/Toxic', 'No Hate/Toxic', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'Hate 1', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 2', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Toxic 1', 'Toxic 1', 'Hate 1', 'Toxic 1', 'No Hate/Toxic', 'Toxic 2', 'Hate 1', 'Hate 1', 'Toxic 2', 'Hate 1', 'No Hate/Toxic', 'Hate 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Toxic 2', 'Toxic 2', 'No Hate/Toxic', 'Hate 1', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'Toxic 2', 'Toxic 1', 'Toxic 3', 'Toxic 2', 'Hate 1', 'No Hate/Toxic']\n"
     ]
    }
   ],
   "source": [
    "file_response = client.files.content(\"file-9su2ZTmfcJlzRj7BgJSCeilW\")\n",
    "\n",
    "# Assuming file_response.text contains the full content with multiple JSON objects, one per line\n",
    "file_content = file_response.text\n",
    "\n",
    "# Split the content into individual lines (assuming each line is a separate JSON object)\n",
    "lines = file_content.strip().splitlines()\n",
    "\n",
    "# Initialize an empty list to store the labels\n",
    "gold_labels = []\n",
    "\n",
    "# Iterate through each line and parse the JSON object\n",
    "for line in lines:\n",
    "    try:\n",
    "        # Parse each line as a JSON object\n",
    "        response = json.loads(line)\n",
    "        # Navigate to the 'content' field\n",
    "        gold_label = response[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        gold_labels.append(gold_label)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e} on line: {line}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in response: {e}\")\n",
    "\n",
    "# Now you have a list of gold labels\n",
    "print(gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richm\\AppData\\Local\\Temp\\ipykernel_22328\\1600656078.py:3: DtypeWarning: Columns (0,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold labels have been assigned for rows 1000 to 1999, index retained.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "file_path = \"clean_concatenated_texts_data_with_labels.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assign labels for rows with index 1000 to 1999\n",
    "df.loc[6000:35999, \"gold_label\"] = gold_labels[:30000]\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file, retaining the index\n",
    "df.to_csv(\"clean_concatenated_texts_data_with_labels.csv\", index=False)\n",
    "\n",
    "print(\"Gold labels have been assigned for rows 1000 to 1999, index retained.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
