{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DSA4264 Team Detoxify Documentation","text":"<p>Members: Joy Tan, Kylie Tan, Koh Yi Jing, Richmond Sin, Sarah Goh</p> <p>Welcome to the documentation for Team Detoxify in DSA4264, where we focus on analyzing and mitigating toxic and hateful online comments. Our project leverages advanced machine learning models to classify harmful content and uncover underlying themes, enabling safer and more constructive digital interactions.</p>"},{"location":"#report-overview","title":"Report Overview","text":"<p>Our technical report is structured as follows:</p>"},{"location":"#1-context","title":"1. Context","text":"<ul> <li>Overview of the problem: the prevalence and impact of toxic and hateful online content.</li> <li>Motivation for the project and its significance in creating safer digital environments.</li> </ul>"},{"location":"#2-scope","title":"2. Scope","text":"<ul> <li>Defines the boundaries of our project, including key objectives and deliverables.</li> <li>Highlights the specific focus areas within toxicity and hate speech classification.</li> </ul>"},{"location":"#3-methodology","title":"3. Methodology","text":""},{"location":"#31-data-processing","title":"3.1 Data Processing","text":"<ul> <li>Data collection and cleaning processes.</li> <li>Description of the datasets used, including their sources, size, and preprocessing steps.</li> </ul>"},{"location":"#32-modellingmulticlass-text-classification","title":"3.2 Modelling/Multiclass Text Classification","text":"<ul> <li>Development and fine-tuning of our Multilingual DistilBERT model.</li> <li>Explanation of the classification categories and metrics used to evaluate model performance.</li> </ul>"},{"location":"#33-modellingtopic-modelling","title":"3.3 Modelling/Topic Modelling","text":"<ul> <li>Application of BERTopic to identify and analyze key themes in toxic comments.</li> <li>Explanation of how BERTopic extracts topics and provides deeper insights into the nature of harmful content.</li> </ul>"},{"location":"#34-application","title":"3.4 Application","text":"<ul> <li>Implementation of the model in a real-world setting through our Streamlit application.</li> <li>User workflows for classifying and analyzing comments, including manual input and bulk analysis via CSV uploads.</li> </ul>"},{"location":"#4-findings","title":"4. Findings","text":"<ul> <li>Key insights and trends derived from the analysis.</li> <li>Visualizations and interpretations of the results, highlighting the effectiveness of our models in detecting and categorizing toxic content.</li> </ul> <p>This documentation provides a comprehensive guide to our methodologies, findings, and applications, serving as a reference for understanding and extending our work.</p>"},{"location":"home/acknowledgements/","title":"Acknowledgements","text":""},{"location":"home/acknowledgements/#thank-you-message-from-team-detoxify","title":"Thank You Message from Team Detoxify","text":"<p>If you\u2019ve chanced upon this part of the technical report or have read this far, we want to express our heartfelt gratitude. Our DSA4264 project, focused on classifying toxic and hate speech on Reddit, has been both a challenging and rewarding experience.</p> <p>We would like to extend our deepest thanks to Professor Shaun Khoo, whose invaluable guidance and expertise have been instrumental throughout this journey. His support has been a constant source of motivation, helping us navigate complex problems and refine our approach to this important issue.</p> <p>This project has allowed us to apply advanced data science techniques to address real-world challenges in online discourse. We are proud of what we\u2019ve accomplished and hope our work contributes to making online spaces safer and more inclusive.</p> <p>To future readers, we hope this documentation provides meaningful insights and serves as a lasting record of our efforts. It stands as a testament to the power of teamwork, determination, and the pursuit of impactful solutions.</p> <p>Thank you once again, Professor Khoo, for your mentorship and support. This report will remain a permanent reminder of the dedication and passion that drove the DSA4264 team.</p> <p>Farewell, and may the pursuit of knowledge and innovation continue to inspire.</p> <p>Warm regards, The DSA4264 Team Detoxify</p>"},{"location":"home/acknowledgements/#project-gallery","title":"Project Gallery","text":"<p>Here are some highlights from our project journey.</p>"},{"location":"home/acknowledgements/#our-last-meeting-at-smu","title":"Our Last Meeting at SMU","text":"<p> Caption: Our last physical meeting before the last stretch (we concluded that SMU students dressed better than us). 31<sup>st</sup> Oct 2024</p>"},{"location":"home/acknowledgements/#first-consultation-with-prof-khoo","title":"First consultation with Prof Khoo","text":"<p> Caption: Changing some of our plans, but unfazed by the challenges. 24<sup>th</sup> Oct 2024</p>"},{"location":"home/acknowledgements/#data-labelling","title":"Data Labelling","text":"<p> Caption: Visualizing the impact of our classification model on test data. 22<sup>nd</sup> Oct 2024</p>"},{"location":"home/acknowledgements/#starbucks-sessions","title":"Starbucks Sessions","text":"<p> Caption: Exploring a different starbucks every week (Novena is the best). 17<sup>th</sup> Oct 2024</p>"},{"location":"home/acknowledgements/#excited-for-the-project","title":"Excited for the Project","text":"<p> Caption: Our first virtual meeting, ironing out our plans. 5<sup>th</sup> Oct 2024</p>"},{"location":"home/acknowledgements/#project-kick-off","title":"Project Kick-Off","text":"<p> Caption: Our first meeting after choosing the project topic. 2<sup>nd</sup> Oct 2024</p>"},{"location":"home/technical_report/appendix/","title":"Appendix","text":"<p>Detailed Analytical Insights and Findings from Topic Modelling:</p>"},{"location":"home/technical_report/appendix/#1-topic-race","title":"1. Topic: Race","text":""},{"location":"home/technical_report/appendix/#11-increase-in-toxicity-in-year-2021-and-2022","title":"1.1. Increase in Toxicity in year 2021 and 2022","text":""},{"location":"home/technical_report/appendix/#111-insights","title":"1.1.1. Insights","text":"<p>The discourse surrounding \"Chinese privilege\" in the context of racial privilege in Singapore was a key factor behind the spike in toxicity in Reddit comments in 2021. Many toxic comments criticized racial policies, such as: \u201cWe have multiple policies like the HDB quota, but this does not benefit minorities.\u201d This statement suggests that while policies like the quota aim to promote racial integration, they may unintentionally disadvantage minorities. Other comments, such as those criticizing an immigration policy that \u201caims to maintain majority privilege,\u201d highlight deeper societal issues and hidden systemic biases that disproportionately affect minority groups. Examples of these challenges include the prevalence of \"SAP schools\" and difficulties in securing jobs due to language barriers, such as the inability to speak Mandarin.</p> <p>Former Prime Minister Lee Hsien Loong also acknowledged the ongoing challenges faced by ethnic minorities in his 2021 National Day Rally, noting that they continue to struggle with issues like renting homes and finding employment.</p> <p>Comments like \u201cSingapore's racial harmony was never real, it was always racial tolerance\u201d raise important questions about the state of community relations, suggesting that true racial integration may still be far from achieved.</p> <p>One key policy under scrutiny is the Ethnic Integration Policy (EIP), which ensures a quota for minority racial groups in housing estates to promote integration. While the policy aims to foster interaction between racial groups, it has inadvertently disadvantaged certain communities, particularly affecting their ability to resell flats. In 2023, Minister K. Shanmugam addressed these concerns, explaining that rather than abolishing the policy, the government would exercise flexibility and respond to the appeals of affected groups. This adjustment contributed to a decrease in toxicity levels in 2023.</p> <p>While targeted policy changes are underway, the persistent online discourse reveals broader societal challenges in addressing invisible racism and fostering true multicultural integration. Former Prime Minister Lee also noted that \u201cnon-Chinese Singaporeans have shared that some Chinese Singaporeans may be unaware of how ethnic minorities feel.\u201d This highlights a critical disconnect in understanding that fuels divisions and perpetuates misunderstandings.</p>"},{"location":"home/technical_report/appendix/#112-solutions","title":"1.1.2. Solutions","text":"<p>The lack of awareness among some members of the majority about the lived experiences of minority groups underscores the need for initiatives aimed at building empathy and cross-cultural understanding.To reduce toxicity surrounding racial privilege, policymakers should consider the following approaches:</p> <p>Policy teams should develop community engagement initiatives that promote interactions across different racial groups. This could potentially enhance understanding and dismantle stereotypes as they develop empathy with people outside of their own race. Alongside this, continuous refinement of policies affecting racial dynamics is crucial. Policy teams should ensure these policies are fair and equitable, supported by constant evaluations and refinements to minimize negative impacts based on citizen\u2019s feedbacks.</p> <p>By fostering empathy and refining policies, Singapore can continue to strengthen its multicultural fabric and build a more inclusive society where all racial groups feel respected and valued.</p>"},{"location":"home/technical_report/appendix/#12-increase-in-hate-from-2021-to-2023","title":"1.2. Increase in Hate from 2021 to 2023","text":""},{"location":"home/technical_report/appendix/#121-insights","title":"1.2.1. Insights","text":"<p>Hate speech targeting Chinese nationals has noticeably increased, influenced by specific incidents as well as scams involving individuals of this nationality. This discourse not only deepens societal divisions but also reinforces negative stereotypes, triggering widespread backlash against Chinese nationals. A notable instance of such speech occurred in response to an altercation involving a Chinese woman at a hospital, where comments such as \u201cGo back to China, you parasitical fuck\u201d were recorded. Further exacerbating the issue, media portrayals often cast Chinese nationals in a negative light, such as the influx of Chinese tourists into NUS compounds and a series of housebreakings attributed to Chinese nationals just recently.</p> <p>Scams of this nationality is also prevalent, in a case where a cleaner lost $330,000 to fraudsters impersonating Chinese police. In response to this, a comment said, \u201cAnything remotely related to Chinese officials and cops should be blocked and hung up immediately. Remind your elderly family members.\u201d The frequency of such scam cases has escalated, with phone-based scams rising to 7,196 incidents in 2023 from 3,602 in 2022. The persistence of these issues into 2024, with victims losing nearly $1 million to con artists claiming to represent Chinese firms and an additional $16.3 million to scammers posing as ICA and Chinese government officials, further tarnishes the public image of Chinese nationals.</p>"},{"location":"home/technical_report/appendix/#122-solutions","title":"1.2.2. Solutions","text":"<p>To address the rise in hate speech against Chinese nationals and spread of unfounded stereotypes, policy teams can partner with media outlets to foster fair and positive representation of all nationalities. This collaboration can include guidelines that emphasize the importance of balanced reporting and the repercussions of racial stereotyping. Media campaigns can also be initiated to highlight the positive contributions of the Chinese community to Singaporean society, counteracting negative portrayals.</p> <p>To combat scam prevention, educating the vulnerable groups, especially the elderly who are more likely to fall prey to such scams is very important. These programs should inform them on the nature of scams, particularly those involving impersonation of officials, which disproportionately stigmatize certain nationalities. Education can include information on how to recognize and report scams as well as the importance of skepticism in dealing with unsolicited communications.</p> <p>Stronger law enforcement responses and legal frameworks also needs to be established to tackle the issue of scams more robustly. This should include the development of international cooperation in law enforcement to tackle cross-border criminal activities, which often involve scams. Additionally, financial institutions should be mandated to implement robust security measures and transaction monitoring systems that can detect and prevent fraudulent activities. Regular audits and updates to these systems should be enforced to adapt to new scamming techniques. By identifying trends and potential threats early, law enforcement and financial institutions can act swiftly to prevent scams before they affect large numbers of people.</p>"},{"location":"home/technical_report/appendix/#2-topic-crimes","title":"2. Topic: Crimes","text":""},{"location":"home/technical_report/appendix/#21-increase-in-toxicity-in-year-2023","title":"2.1. Increase in Toxicity in year 2023","text":""},{"location":"home/technical_report/appendix/#211-insights","title":"2.1.1. Insights","text":"<p>The analysis of toxic comments related to criminal cases on social media reveals a prevailing trend of extreme anger and demands for severe punishment, particularly in response to sexual crimes, resulting in high number of toxic 3 comments. This pattern is evident in reactions to several high-profile incidents that have captured public and media attention.</p> <p>In one notable case, public outrage was sparked by an incident where four husbands conspired to drug and rape each other\u2019s wives, a crime so heinous that it garnered international media coverage. Comments such as \u201cImagine making children with someone who ends up arranging for u to b raped... with your kids in the other room I need to see caning!!\u201d exemplify the intense emotional responses and calls for harsh punitive measures from the community.</p> <p>Similarly, another distressing case involved a man who sexually abused his daughter for over a decade, beginning when she was only eight years old. The severity of this betrayal elicited comments like \u201cHis own daughter\u2026Humans should all be fucking wiped out. I'm sorely disappointed in Covid,\u201d reflecting a profound disillusionment with humanity. The discussion threads for this case were filled with vehement language, including demands to \u201cHang him\u201d, highlighting a collective sentiment focused on retribution rather than addressing underlying cause of crime.</p> <p>Overall, the discourse surrounding these criminal activities is characterized by a punitive tone that emphasizes severe penalties. This response pattern indicates a societal tendency to prioritize retribution, thus there is a critical need for a balanced approach to criminal justice that includes effective punishment and preventive measures.</p>"},{"location":"home/technical_report/appendix/#212-solutions","title":"2.1.2. Solutions","text":"<p>Policy teams can have campaigns aimed at raising awareness about the causes and consequences of sexual crimes. These campaigns should aim to shift public perception towards a more nuanced understanding of factors driving criminal behavior. Educating the public on the benefits of rehabilitation and the potential for recidivism reduction through appropriate therapeutic interventions could help balance the discourse.</p> <p>Policy team can also ensure that sentencing guidelines for sexual crimes and other severe offenses are appropriately stringent but also allow for judicial discretion based on the specifics of each case. Sentencing reforms should be publicly communicated to assure the community that justice is being served while also emphasizing the role of rehabilitation.</p>"},{"location":"home/technical_report/appendix/#22-increase-in-hate-from-2020-to-2023","title":"2.2. Increase in Hate from 2020 to 2023","text":""},{"location":"home/technical_report/appendix/#221-youth-vaping","title":"2.2.1. Youth Vaping","text":""},{"location":"home/technical_report/appendix/#2211-insights","title":"2.2.1.1. Insights","text":"<p>Firstly, there is a substantial number of hateful comments directed at youths that vapes. Vaping among the youth in Singapore has seen a significant rise, as indicated by the stringent enforcement actions and extensive media coverage dedicated to this issue. In the first half of 2024, approximately 5,480 individuals were apprehended for using or possessing vapes, with about 20 percent of these individuals being students. The Ministry of Health and Health Sciences Authority have reported a 52 percent increase in fines for e-vaporizer offenses compared to the previous quarter, underscoring the growing concern over youth vaping. The public\u2019s reaction to youth vaping often adopts a punitive tone, with many community members calling for severe enforcement measures. Comments such as \u201cYa, don\u2019t ruin it for others. Once Singapore Police Force think there\u2019s too many this kind of people implying not enough enforcement they\u2019re gonna come down hard, which applies to not just vapes but most stupid laws in Singapore,\u201d reflect a broader societal frustration with behaviors perceived as flouting laws, exacerbating the stigmatization of young individuals engaged in vaping. This sentiment suggests that if vaping is done discreetly, it might escape legal scrutiny, thus avoiding drawing further attention from law enforcement which could lead to more severe crackdowns on this behavior across the board.</p> <p>This perspective indicates an understanding among some segments of the public that while the activity remains illegal, the visibility of the offense significantly influences the severity of police response. The comment underscores a pragmatic approach to the reality of ongoing vaping among youths\u2014suggesting that while they might continue to engage in this behavior, doing so less conspicuously could mitigate the risks of harsh penalties. Such an attitude can complicate efforts to enforce anti-vaping laws and highlights the challenge authorities face in ensuring compliance not through fear of increased enforcement alone but through broader cultural shifts and understanding of the laws\u2019 purposes. Several factors drive the prevalence of vaping among youth, including peer pressure and the desire for social acceptance, stress and personal issues, and curiosity about new experiences. However, current measures to curb youth vaping have primarily focused on punitive actions rather than addressing these underlying causes.</p>"},{"location":"home/technical_report/appendix/#2212-solutions","title":"2.2.1.2. Solutions","text":"<p>The policy team can launch educational initiatives that extend beyond the risks of vaping to address the psychological and social reasons why youths may start vaping. These campaigns should aim to equip youths with better stress management techniques, enhance their ability to resist peer pressure, and provide them with healthier alternatives to cope with personal issues.</p> <p>Additionally, adjusting legal penalties to incorporate rehabilitative measures rather than solely punitive responses can significantly change the enforcement landscape. This approach should focus on integrating educational penalties that require young offenders to attend programs that highlight the dangers of vaping, coupled with support for cessation efforts, such as counseling or therapy. By emphasizing rehabilitation and education within the legal framework, policy can shift from a punitive to a corrective model that better addresses the needs of youths and encourages long-term behavioral change. This balanced approach not only serves to deter vaping but also supports the overall well-being and development of young individuals, steering them away from future offenses.</p>"},{"location":"home/technical_report/appendix/#222-crimes-by-youths","title":"2.2.2. Crimes by Youths","text":""},{"location":"home/technical_report/appendix/#2221-insights","title":"2.2.2.1. Insights","text":"<p>The public discourse surrounding hate crimes committed by youths, especially in cases of sexual offenses, reflects a complex mix of concern, outrage and a demand for harsher penalties. For instance, in a notable legal adjustment, the High Court increased the jail time for sex offenders from Nanyang Technological University (NTU) after an uproar over the initial light sentencing, and controversial defense tactics that blamed the young victim. Such judicial actions are often a response to public sentiment, as evidenced by comments like \u201cVery light sentence for a predatory pedophile.\u201d, reflecting a collective dissatisfaction and a call for justice that aligns more closely with the gravity of offenses committed. Furthermore, the increase in youth arrests for sexual crimes, with a noted 30% jump in 2023 alone. For example, the case where a youth was charged for molesting his niece was attributed to his addiction to hentai, as reported. This incident sparked comments such as \u201cHe wasn\u2019t diagnosed with a mental illness, just offered this bullshit excuse for molesting a 6 year old girl,\u201d, highlighting public frustration over perceived inadequacy of reasons provided for such behaviour. Compounding the issue is the perception of insufficient legal consequences for young offenders, as seen in cases like the group assault led by a minor on a victim with low IQ, where the sentence was deemed too light by public commentators. This perception often leads to heightened frustrations and calls for more severe legal actions, reflecting a broader societal struggle to balance the rehabilitation needs of young offenders with the community\u2019s desire for justice.</p>"},{"location":"home/technical_report/appendix/#2222-solutions-for-crimes-by-youths","title":"2.2.2.2. Solutions for \"Crimes by Youths\"","text":"<p>The policy team can look at enhancing the judicial response to youth-related sexual offenses. This could involve revisiting sentencing guidelines to ensure they reflect both the severity of the crimes and the potential for rehabilitation. To adequately address the community\u2019s demand for justice, the policy team could recommend the implementation of mandatory minimum sentences for certain types of severe sexual offenses committed by youth. Alongside this, the introduction of enhanced victim support services during court proceedings could help alleviate some of the community concerns about the treatment of victims during trials. These services would ensure that victims are not only protected but also respectfully heard, helping to restore public faith in the justice system. Enhancing rehabilitation programs for youth offenders is essential to reduce recidivism and integrate these individuals back into society successfully. The policy team should advocate for specialized rehabilitation facilities that provide psychological counseling, educational support, and vocational training tailored to the needs of young offenders. These facilities should operate under the principle that early intervention can significantly alter the life paths of these youths. Furthermore, community-based programs that allow for supervised integration of offenders, coupled with continuous monitoring and support, can help these individuals rebuild their lives and mend the social fabric disrupted by their actions. These initiatives must be accompanied by public awareness campaigns that educate the community about the benefits of rehabilitation, aiming to shift the narrative from one of retribution to one of restoration and prevention.</p>"},{"location":"home/technical_report/appendix/#3-topic-gender","title":"3. Topic: Gender","text":"<p>There has generally been a decrease in toxicity and hate levels surrounding the issue of gender in Singapore. This shows a shift away from the traditional patriarchal mindset, and progress towards gender equality in Singapore, with Singapore ranked 7<sup>th</sup> in the Gender Equality Index in 2020 and 8<sup>th</sup> in 2024 out of 166 countries. This signifies that Singapore has made further progress in areas such as equal opportunities in the workplace, providing greater caregiver support and strengthening protection against violence and online harms.</p> <p>However, there was an anomaly in 2022, where toxic and hate levels saw an unexpected spike. This deviation highlights that while overall trends are improving, certain periods or specific events can still exacerbate negative sentiment, requiring ongoing attention and intervention.</p>"},{"location":"home/technical_report/appendix/#31-increase-in-toxic-gender-in-year-2022","title":"3.1. Increase in Toxic Gender in year 2022","text":""},{"location":"home/technical_report/appendix/#311-insights","title":"3.1.1. Insights","text":"<p>One of the key reasons for the anomaly in 2022 was the heated discussions surrounding the potential inclusion of females in National Service (NS). This debate stirred significant controversy, especially among male counterparts who expressed unhappiness over having to serve for two years, while women were not required to do so. The discourse reflected deep-seated views on gender equality, particularly around civic duties and fairness. Toxic comments in this realm often revolved around perceived inequalities, with many arguing that the imposition of NS on women would somehow alleviate the burden on men, despite the existing pressures they face. For example, one comment read, \"Definitely agree with your view about females serving NS. Doesn't mean that I served so I want to drag in females to do the same shit I did for 2 years. I'd rather they not and go live life.\" Another comment expressed frustration, stating, \"The Minister of Defence stated that making NS compulsory for women outweighs the benefits,\" implying that the Ministry of Defence (MINDEF) takes away men\u2019s benefits for the collective good, further fueling discontent.</p> <p>This was with regards to the statement made by Mr Ng Eng Hen, the Defence Minister, who had mentioned in parliament that \u2018The \u201csocietal cost\u201d of enlisting women into National Service (NS) now will \u201cfar outweigh\u201d the benefits\u2019. There had been suggestions to enlist women for reasons of gender equality, in roles like nurses or teachers.</p> <p>Such sentiments are often rooted in perceptions that the current system disproportionately favors women, sparking feelings of resentment and envy, particularly among those who feel that their sacrifice during National Service is undervalued. This discourse reflects broader societal debates on gender roles and expectations, with some arguing that the lack of female participation in NS exacerbates gender inequality in terms of shared societal responsibilities.</p> <p>The toxic speech surrounding this issue is further compounded by prevailing gender stereotypes. For example, the idea that \"many people, especially men, do not have a good emotional support system because society conditions men to 'deal with it' or 'forget it,'\" contributes to the narrative of toxic masculinity, where men are expected to suppress their emotions, which in turn perpetuates unhealthy standards of emotional resilience.</p>"},{"location":"home/technical_report/appendix/#312-solutions","title":"3.1.2. Solutions","text":"<p>Currently, Singapore has made strides in including women in defense roles, such as through positions in the Singapore Armed Forces (SAF), police, and civil defense. Initiatives like the SAF Volunteer Corps allow women to contribute to national defense, fostering inclusivity in areas once primarily male-dominated. However, a broader public understanding of these contributions and the shared responsibilities of citizenship is essential. By educating citizens on the varied roles both men and women play in national defense and other sectors, policy makers can promote a balanced view of civic duty and recognize the value of diverse contributions.</p> <p>Additionally, increased public dialogue on gender roles could be beneficial, especially in regard to National Service (NS). Public forums, community discussions, or online platforms where both men and women can express their perspectives on NS and broader societal expectations would help bridge understanding. For instance, some feel that NS policies currently place an unequal burden on men, while others emphasize the importance of gender-inclusive policies across all sectors. Creating spaces for open, empathetic dialogue on these topics would foster understanding, address perceived policy imbalances, and highlight how both genders contribute uniquely to Singapore\u2019s growth.</p> <p>There could also be initiatives to support emotional well-being for men. Expanding mental health support and advocating for emotional resilience for men can help break the stigma around vulnerability. Addressing toxic masculinity by promoting healthier emotional expression would reduce the frustration and discontent that contributes to negative discourse.</p>"},{"location":"home/technical_report/appendix/#32-increase-in-hate-gender-in-year-2022","title":"3.2. Increase in Hate Gender in year 2022","text":""},{"location":"home/technical_report/appendix/#321-insights","title":"3.2.1. Insights","text":"<p>The discourse surrounding gender in Singapore reveals a complex interplay between the advancement of feminist movements and the simultaneous rise in misogynistic attitudes. As feminism gains momentum, advocating for women\u2019s rights and equality, there is a notable backlash from certain segments of society who perceive these efforts as promoting female privilege over gender equality. Comments like \u201ctell me, can women build, clean, maintain and lead things without men? Doubt so as they have less physical strength and rely more on emotions than facts\u201d exemplify this resistance. Such sentiments are exacerbated by the perception that while organizations like the Association of Women for Action and Research (AWARE) actively support women\u2019s rights, there is no equivalent advocacy for men\u2019s rights, leading to feelings of neglect and frustration among some men. Additionally, criticisms like \u201cFor all the equality that women want, everything they do something that is traditionally done by men, they want a standing ovation for it. What kind of equality is that\u201d and \u201cEnough about women\u2019s rights, time to talk women\u2019s wrongs\u201d further highlight the cynicism and hostility towards the perceived preferential treatment of women. This gender discourse often intersects with national issues such as the debate over national service, where the question of women\u2019s roles intensifies the gender-related grievances, fueling the ongoing tension between advancing gender equality and addressing perceived imbalances in gender advocacy.</p>"},{"location":"home/technical_report/appendix/#322-solutions","title":"3.2.2. Solutions","text":"<p>In order to combat this hate speech, establishing forums for open dialogue about gender issues can help bridge the divide and address misconceptions directly. These forums could be community-driven events, workshops, or online platforms where individuals from different backgrounds can discuss their views, experiences, and concerns about gender roles and rights. By facilitating these discussions in a structured and respectful environment, the community can move towards a more empathetic understanding of the challenges faced by all genders. It is crucial that these platforms include voices from diverse gender perspectives, ensuring that men\u2019s issues and concerns are also heard and addressed, thus preventing feelings of exclusion or opposition to women\u2019s rights advancements.</p> <p>Additionally, enhancing support for gender advocacy organizations that represent all genders can help balance the perceived focus of existing movements. While organizations like AWARE provide critical support for women, creating or funding similar bodies that can advocate for men\u2019s issues\u2014focusing on mental health, legal rights, or societal roles\u2014could alleviate the sentiment that men\u2019s challenges are overlooked. Policy initiatives could also encourage existing gender-focused organizations to broaden their advocacy to encompass all gender-related issues, promoting a holistic approach to gender equality that recognizes and addresses the unique challenges faced by each gender.</p> <p>In totality, policy teams should collaborate with social media platforms like reddit to develop and implement technologies that can detect and mitigate hate speech and toxic speech in real-time.</p>"},{"location":"home/technical_report/context/","title":"Context","text":""},{"location":"home/technical_report/context/#1-overview-of-online-safety-concerns-on-social-media","title":"1. Overview of Online Safety Concerns on Social Media","text":"<p>In recent years, social media platforms have increasingly become spaces where discussions can become toxic and polarised. This phenomenon poses significant risks, particularly in multicultural societies like Singapore, where diverse viewpoints can clash more intensely in online spaces. The widespread use of social media and the anonymous nature of some platforms, like Reddit, can amplify such hatefulness and toxicity, affecting the digital experience for many users and potentially influencing social cohesion.</p>"},{"location":"home/technical_report/context/#2-growing-concerns-in-singapore","title":"2. Growing Concerns in Singapore","text":"<p>Singapore, as a multi-racial, multi-religious, and highly connected society, faces unique challenges in managing the effects of hate speech and toxicity on social media. The increasing accessibility of the internet to younger users adds urgency to this issue, as children and adolescents are particularly vulnerable to harmful online content. An Online Safety Poll conducted by the Ministry of Digital Development and Information (MDDI) in April 2024 (Figure 1) underscores this, showing a concerning rise in harmful content exposure, with 66% of respondents encountering harmful content, up from 57% the previous year. Therefore, there is a growing concern and hence an urgent need to curb the issue of rising hate and toxic speech in the online environment.</p> <p></p>   Figure 1. News article on Online Safety Poll conducted by MDDI"},{"location":"home/technical_report/findings/","title":"Findings","text":""},{"location":"home/technical_report/findings/#1-results","title":"1. Results","text":""},{"location":"home/technical_report/findings/#11-classification-model-results","title":"1.1 Classification Model Results","text":"Ridge Ridge 5k XGBoost DistilBERT old DistilBERT 5k DistilBERT No Hate/Toxic 0.67 0.61 0.65 0.45 0.63 0.72 Toxic 1 0.57 0.54 0.52 0.21 0.36 0.61 Toxic 2 0.78 0.40 0.42 0.34 0.39 0.79 Toxic 3 0.99 0.60 0.68 0.08 0.98 0.98 Hate 1 0.74 0.70 0.73 0.40 0.77 0.79 Hate 2 0.97 0.87 0.91 0.60 0.94 0.97 Hate 3 0.99 0.99 0.94 0.57 0.99 0.99 Macro Avg F1-score 0.81 0.72 0.81 0.39 0.72 0.84   Table 3: Model Experimentation Results comparing macro-average F1-scores between different classifier models. Model names with \"5k\" indicate that the model was trained on a smaller dataset containing less synthetic data that we generated to balance the classes. DistilBERT old was trained on data labelled with a previous version of class defintions that made classes less distinct from each other.   <p>DistilBERT achieved the highest F1-score (0.84), making it the top-performing model. Models trained on the smaller 5k dataset (a mix of synthetic and actual labeled data), and in certain synthetically generated classes showed lower F1-scores, indicating that the reduced dataset may lack the nuanced variations found in real-world text, which are crucial for effective hate and toxic speech detection. DistilBERT old, trained with a less precise set of class definitions, had the lowest macro-average F1-score (0.39), underscoring the importance of the updated, distinct class definitions in improving the training performance of our classifier.</p>"},{"location":"home/technical_report/findings/#12-topic-modelling-results","title":"1.2 Topic Modelling Results","text":"<p>Topic modeling identified rising themes in hate and toxicity across key topics: race, crime, and gender. More in-depth analysis can be found here.</p> <p>Race has emerged as the most pressing issue among the three categories analyzed, with the highest increase in both Net Hate and Net Toxicity Scores, reflecting a significant rise in intensity levels up to 2024. This issue has proven to be both recurring and deeply rooted in society. Given that many of the problems identified are ongoing, policymakers must prioritize addressing them. Additionally, a large portion of high-intensity comments stems from this topic, underscoring the need for targeted interventions to tackle the growing hate and toxicity in this area. Graph showing Net Hate and Toxicity trends of the 3 Topics from 2020-2023.</p> <p>This methodological approach, utilizing advanced topic modeling techniques and thematic analysis, serves to reveal insights into the shifting landscapes of discourse on Reddit. By examining the frequency and context of topics associated with toxic and hateful sentiments, we aim to use our methodology to uncover the underlying trends and catalysts contributing to the increase in toxicity and hate in social media on a larger scale in future.</p> <p></p>   Graph showing Net Hate and Toxicity trends of the 3 Topics from 2020-2023   <p>This methodological approach, utilizing advanced topic modeling techniques and thematic analysis, serves to reveal insights into the shifting landscapes of discourse on Reddit. By examining the frequency and context of topics associated with toxic and hateful sentiments, we aim to use our methodology to uncover the underlying trends and catalysts contributing to the increase in toxicity n hate in social media on a larger scale in future.</p>"},{"location":"home/technical_report/findings/#2-discussion","title":"2. Discussion","text":""},{"location":"home/technical_report/findings/#21-classtification-modelling-insights","title":"2.1 Classtification Modelling Insights","text":"<p>The results provide valuable insights for safeguarding digital safety, with the multiclass text classifier effectively classifying hate and toxicity based on intensity. Topic modeling further contextualizes the classifier\u2019s findings, highlighting specific social issues that drive hate and toxicity\u2014information that can guide targeted intervention and more nuanced policy solutions.</p> <p>For MDDI\u2019s Online Trust and Safety department, these insights enable actionable responses to the key themes and comments identified as severe, directly from our data science solutions. The results from our methods go beyond capturing increased frequency of hateful and toxic topics, to further prioritize topics based on intensity metrics that capture heightened hate and toxicity using our measures to calculate intensity.</p> <p>However, certain limitations remain that influence the model\u2019s real-world effectiveness and should be considered when deploying the solution:</p> <p>Class Imbalance: Despite efforts to address class imbalance through oversampling, SMOTE, and class weighting, categories such as Hate 3 (threats and violence) and Toxic 3 (harassment) remained underrepresented. This imbalance affects the model\u2019s ability to accurately classify these minority classes, leading to lower performance metrics in critical areas of concern. For MDDI, this translates to potential risks in effectively capturing the most severe forms of online harm, where a single undetected instance of hate or harassment may have significant negative impact. Hence, we approached the problem with synthetic data generation.</p> <p>Singlish and Language Nuances: The diverse linguistic patterns in Singlish posed unique challenges. Non-standard grammar and culturally-specific terms were often misclassified. However, we tried to mitigate this issue by using multilingual distilBERT that works well on the individual languages. This challenge limits the model\u2019s interpretability and accuracy for multilingual communities and highlights the importance of adapting language models to language and cultural nuances, which could value-add by improving precision in such contexts.</p> <p>Complexity of Analysis: Comments that may fall into more than one topic category were only put them into one through BERTopic. This could cause relevant comments to be left out, limiting the precision of our intervention. Through the items we showcase in our frontend application, LionGuard Pro Max, we aim to enable clarity to users by displaying actual comments within each cluster.</p> <p>Granularity of Analysis: The model, while effective in broadly classifying hate and toxic content, lacks the capacity to provide subgroup-level insights. This limitation restricts MDDI's ability to drill down into specific user subgroups, such as frequent offenders or communities where harmful content is concentrated, reducing the precision of targeted interventions. An enhancement that segments comments by user or community could support more customized approaches to harm reduction, enhancing digital safety.</p> <p>Enhanced interpretability would allow MDDI to better understand model outcomes, while addressing class imbalance and cultural language nuances would improve fairness in content moderation. A continued focus on addressing these areas could lead to more reliable and equitable digital safety solutions in future iterations.</p>"},{"location":"home/technical_report/findings/#22-topic-modelling-insights","title":"2.2 Topic Modelling Insights","text":"<p>Following our analysis of hate and toxic trends, we identified three pressing issues: gender biases, racial tensions, and youth-involved crimes. These concerns reflect broader societal challenges tied to deeply embedded perceptions.</p> <p>The rise of \u201cracial privilege\u201d highlights systemic advantages for the Chinese majority, sparking debates about inclusivity and causing increased toxicity trends. There has also been a rise in xenophobia, particularly towards Chinese nationals, further exacerbating racial tensions. Calls for greater inclusivity and equal representation have been growing, as many feel that minority voices and needs are not adequately addressed.</p> <p>Similarly, the issue of sexual and youth-related hate crimes has become a heated topic, with the public criticizing perceived leniency in sentencing. Instances of youth offenders receiving light sentences for serious offenses, such as sexual crimes, have led to public calls for stronger judicial responses. There is a widespread demand for the legal system to align more closely with the severity of these crimes to ensure that justice is served and to prevent further harm.</p> <p>In terms of gender issues, the toxicity surrounding gender discussions had been on the decline until recently, when dissatisfaction surrounding National Service (NS) and toxic masculinity led to increased negative discourse. Public frustration grew over gender perceptions and stereotypes, with some expressing resentment towards gender equality movements. This fueled hate and gender biases, particularly targeting women and reinforcing harmful gender stereotypes.</p>"},{"location":"home/technical_report/findings/#3-recommendations","title":"3. Recommendations","text":""},{"location":"home/technical_report/findings/#31-improvements-for-classification-modelling","title":"3.1 Improvements for Classification Modelling","text":"<p>To enhance and expand our solution for MDDI's Online Trust and Safety initiatives, we propose several future directions that address current limitations and broaden the utility of the model:</p> <ol> <li>Model Improvements: Fine-tuning the multilingual DistilBERT model with more diverse datasets could improve its sensitivity to nuances like Singlish and help address the underrepresentation of certain toxic categories. Additionally, exploring more advanced transformer-based models, such as XLM-RoBERTa, could strengthen performance across multilingual contexts and provide better classification accuracy, particularly for minority classes like Hate 3 and Toxic 3.</li> <li>Broader Application: Extending the model's applicability beyond Reddit to other platforms, such as Twitter, Facebook, and YouTube, would offer a broader perspective on harmful content trends and enhance the model's ability to capture patterns across various social media ecosystems. This cross-platform approach would enable MDDI to monitor and respond to toxic behavior trends more effectively, creating a more cohesive digital safety strategy.</li> <li>Explainability: Integrating explainable AI techniques would provide clearer insights into the reasons behind each classification. This added interpretability would help moderators, policymakers, and other stakeholders trust and utilize the model as a decision-support tool, reducing the opacity of the \"black-box\" nature often associated with machine learning models.</li> <li>Real-Time Monitoring and Intervention: Developing capabilities for real-time monitoring would enable the system to flag and address harmful content immediately as it\u2019s posted. This proactive approach would allow for timely interventions, such as issuing warnings or suspending accounts, thereby preventing the escalation of toxic discussions and providing a safer online environment.</li> </ol> <p>These recommended directions align with the goals of enhancing interpretability, fairness, and deployability, offering MDDI a scalable path forward in adapting the solution to meet emerging challenges in online safety.</p>"},{"location":"home/technical_report/findings/#32-recommendations-based-on-topic-modelling","title":"3.2 Recommendations based on Topic Modelling","text":"<p>To address race, crime and gender being the most pressing issues, we propose several solutions in tackling hate and toxic speech.</p> <p>For race related recommendations, it is essential to enhance community engagement by developing and implementing initiatives that foster cross-cultural understanding and empathy among different racial groups. This approach aims to reduce the toxicity often associated with racial discussions. Simultaneously, there is a need for continuous evaluation and refinement of policies such as the Ethnic Integration Policy (EIP). This can be achieved through collaboration with the relevant ministries. By ensuring these policies are fair and responsive to the needs of all communities, we can minimize the unintended negative impacts that may arise, thus allowing us to progress towards a more inclusive and harmonious society.</p> <p>For crime related recommendations, educational campaigns that educate the public about cause and consequences of crimes, especially sexual crimes, are important. These campaigns should aim to shift the focus from a retribution-centric approach to one that emphasizes rehabilitation and prevention. Additionally, there is a need for judicial and sentencing reforms to adjust guidelines in a way that balances the severity of punishment with the potential for rehabilitation. Such reforms will ensure justice is served while also fostering opportunities for offenders\u2019 reintegration into society, ultimately contributing to a safer and more understanding community.</p> <p>For gender related recommendations, promoting inclusive dialogues is key. Facilitating open discussions to address misconceptions about gender roles and National Service can help highlight the contributions of all genders to national development. Alongside this, it is vital to implement initiatives that support emotional well-being, especially for men. Addressing issues of toxic masculinity and promoting healthier emotional expression can foster a more supportive environment where everyone feels valued and understood. Together, these efforts can pave the way for a more gender-inclusive society.</p> <p>Together, these recommendations aim to promote understanding, empathy, and inclusivity in addressing race, crime, and gender issues, ultimately contributing to a more cohesive and supportive society.</p>"},{"location":"home/technical_report/scope/","title":"Scope","text":""},{"location":"home/technical_report/scope/#1-problem","title":"1. Problem","text":""},{"location":"home/technical_report/scope/#11-problem-faced-by-mddi","title":"1.1 Problem Faced by MDDI","text":"<p>The Online Trust and Safety Department is tasked with safeguarding the digital space for Singaporean users, particularly children and vulnerable groups. However, the team currently lacks concrete data on the scale and evolution of hate and toxic behaviour on Singapore-focused subreddits. Without a detailed, historical analysis of trends in online toxicity, it is challenging for MDDI to understand the extent of the issue or to identify the specific factors driving increased hatefulness and toxicity. This lack of data impedes the department\u2019s ability to design targeted interventions and effectively engage with social media platforms to mitigate risks.</p>"},{"location":"home/technical_report/scope/#12-significance-of-the-problem","title":"1.2 Significance of the Problem","text":"<p>The rise in online hatefulness and toxicity poses significant social risks, particularly for a diverse society like Singapore, where online discourse can influence public sentiment and community relations. The Online Safety Poll indicates an upward trend in harmful content exposure, with potential consequences for youth and societal cohesion. Not addressing this problem could lead to increased polarisation, a diminished sense of safety in online spaces, and potentially a negative impact on public trust. Metrics include the reported increase in harmful content exposure (from 57% to 66% of users within a year), with particular focus on high-risk age groups and vulnerable populations.</p>"},{"location":"home/technical_report/scope/#13-why-a-data-science-approach-is-essential","title":"1.3 Why a Data Science Approach is Essential","text":"<p>Given the massive volume of data and the nuances of language, manual review is impractical for identifying trends in online hate and toxicity across the years. Data science enables scalable, data-driven analysis, and leveraging Natural Language Processing (NLP) techniques helps MDDI to automate the detection and classification of hate and toxic comments, enabling the extraction of actionable insights at scale. By analyzing comment-level data over time, MDDI can gain insights into the extent of the issue, providing them with an efficient means of tracking trends over time and identifying themes associated with heightened hatefulness and toxicity on social media.</p>"},{"location":"home/technical_report/scope/#2-success-criteria","title":"2. Success Criteria","text":"<p>Should this project be successful, the following business and operational goals will be met:</p> <p>Insightful Reporting for Policy Development: Generate clear, data-backed insights on hate and toxic speech in the online environment, by observing trends over the recent years (2020 to 2023) and the specific factors and subjects involved. These meaningful insights will be able to support MDDI in their policy recommendations and interventions to curb the problem in a precise manner. MDDI can then implement actionable strategies, including the engagement of various stakeholders like social media companies, to aid in reducing the exposure of such of these harmful content.</p> <p>Automated Hatefulness/Toxicity Detection Pipeline: Develop a reliable pipeline that automates the processing and classification of Reddit comments by hatefulness or toxicity levels. This automation will be able to read in a Reddit comment, before classifying it into one of the 7 categories under the Language Intensity Classification system defined by us. This system will increase MDDI\u2019s capacity to monitor and analyse social media content efficiently, with minimal manual intervention. In addition, with the classification of comments into the 7 categories, MDDI can better understand the change in intensity of online hate/toxicity trends over the years, the causes driving these trends and ultimately implement targeted solutions.</p> <p>Enhanced Public Trust and Safety: After the completion of detailed data-driven analysis, tailored strategies and recommendations can be provided to social media companies to create a safer digital environment, particularly for youths. Success in this goal will be measured by increased collaboration between MDDI and social media platforms on reducing harmful content on Singapore-focused subreddits.</p>"},{"location":"home/technical_report/scope/#3-assumptions","title":"3. Assumptions","text":"<p>Availability of Sufficient Historical Data: The project assumes that the provided Reddit comment data will be sufficient in volume and time span (2020 to 2023) to detect meaningful trends in online toxicity. If data gaps or limitations arise, the scope and depth of trend analyses may be affected.</p> <p>Accuracy of NLP Models in Capturing Nuanced Toxicity: The analysis assumes that NLP models used for hate speech and toxicity detection can accurately capture nuanced and context-specific language that may be uniquely reflective of the Singaporean context. Inaccuracies or model limitations could affect the reliability of toxicity assessments.</p> <p>Engagement from Social Media Platforms: The project\u2019s success partially depends on the willingness of social media companies to consider and act upon MDDI\u2019s findings and recommendations. Limited cooperation from these stakeholders could reduce the overall impact of policy recommendations derived from the project.</p>"},{"location":"home/technical_report/methodology/","title":"Methods Overview","text":"Figure 2. Methodology workflow   <p>The methodology section consists of 5 different sections (Figure 2). In Methods Overview, we first outline the overarching approach to our project, detailing the rationale behind our intensity-based classification system and the steps taken to maximize our analysis with the computational resources we have.</p> <p>Our analysis of Reddit comments aimed to identify trends in hate and toxic speech over time, with a focus on understanding the intensity of such language. We developed a custom multiclass classification framework to go beyond binary distinctions, providing a deeper understanding of how hateful and toxic content manifests. We subsequently deep-dived to analyse hate and toxic speech that were prioritised and had greater urgency to be address using topic modelling. To ensure maintainability and reproducibility, we used Kedro (version 0.19.8) to organize our data science workflows efficiently.</p>"},{"location":"home/technical_report/methodology/#1-technical-assumptions","title":"1. Technical Assumptions","text":"<p>The core assumptions driving our model development, particularly those linked to our novel classification approach and data handling include:</p> <p>Custom Class Definitions: We intentionally designed a multiclass framework to capture varying intensities of hate and toxic speech. Our classifications are intended to differentiate between sensitive group-directed hate speech (Hate 1, Hate 2, Hate 3) and non-sensitive toxic speech (Toxic 1, Toxic 2, Toxic 3). This approach allows for a nuanced analysis of the severity and impact of different text. The definitions shown below were meticulously crafted to account for both subtle and overt forms of disrespect and harm.</p> <p>Feature Availability: We used text content as our primary feature, enhanced with embeddings from multilingual DistilBERT to handle Singapore's linguistic diversity. This decision was critical to capturing the multilingual nature of our dataset, balancing resource limitations with the need for comprehensive language coverage.</p> <p>Computational Resources: Due to limited access to GPUs, we opted for DistilBERT over a full BERT classification model, which still allowed us to conduct efficient analysis. We restricted our analysis on text with length inclusive of 5 to 50 words, and found this reasonable since it was inclusive of the typical Reddit post length.</p> <p>Hypotheses of Interest: We hypothesized that both the frequency and intensity of hate and toxic language would increase over time, with intensity measured according to our custom-defined categories.</p> <p>Data Quality: Our dataset varied in quality, requiring rigorous preprocessing to manage noise, inconsistencies, and class imbalances. Due to the large size of the original dataset, we assume that our measures taken to obtain a balanced subset for analysis is justified.</p>"},{"location":"home/technical_report/methodology/#2-class-definitions","title":"2. Class Definitions","text":"<p>Some works adopt the concept of hate speech as being aimed at a protected group. Additionally, hate and toxic speech need to be identified in a way that prioritizes those needing immediate attention. This led us to the development of a multiclass classification framework:</p>"},{"location":"home/technical_report/methodology/#21-sensitive-group-classification","title":"2.1 Sensitive Group Classification","text":"<p>In our project, we classify a comment as referring to a sensitive group if it mentions characteristics such as nationality, race, ethnicity, religion, gender, sexual orientation, disability, skin color, age, generational group, socioeconomic status, or immigration status. Our choice of groups aligns with Singapore\u2019s legal framework against harmful speech, specifically referencing the Maintenance of Religious Harmony Act and Section 298A of the Penal Code, which identify protected groups. Additionally, based on analysis of sample texts and discussions, we included extra groups that appeared frequently in potentially hateful comments. Text mentioning any of these characteristics explicitly will be classified as Hateful rather than Toxic.</p>"},{"location":"home/technical_report/methodology/#22-language-intensity-classification","title":"2.2 Language Intensity Classification","text":"<p>Subsequently, text will be classified into one of the 7 categories below:</p> <ol> <li>Hate 1: Text containing bias or generalizations without exclusion or incitement.</li> <li>Hate 2: Text that add exclusion or denial of rights to sensitive groups, including denial of access, rights, or opportunities based on group characteristics.</li> <li>Hate 3: Text that incite harm or make threats.</li> <li>Toxic 1: Text containing mild complaints or disrespect without direct insults or threats.</li> <li>Toxic 2: Text containing clear insults, sarcasm, or mocking, focusing on humiliation or belittling.</li> <li>Toxic 3: Text containing harassment or encouragement of harm.</li> <li>No Hate/Toxic: Neutral or constructive comments, including mild criticism or playful sarcasm.</li> </ol>"},{"location":"home/technical_report/methodology/application/","title":"Application","text":"<p>The following sections showcase the different pages and features of LionGuard Pro Max.</p>"},{"location":"home/technical_report/methodology/application/#1-home-page","title":"1. Home Page","text":"<p>The homepage serves as a central hub where users can learn about the application's goals and key features. It includes a left-side navigation bar with buttons for Home, Classify Text, and Analysis, allowing for seamless navigation throughout the application.</p> <p></p>"},{"location":"home/technical_report/methodology/application/#2-classify-text-page","title":"2. Classify Text Page","text":"<p>The Classify Text page lets users classify text manually or by uploading a CSV for batch testing. Users can also view past classifications and learn more about the model used.</p>"},{"location":"home/technical_report/methodology/application/#21-classifying-text-by-manual-classification","title":"2.1 Classifying Text by Manual Classification","text":"<p>The Manual Classification tab on the Classify Text page allows users to type a comment into the chatbox and click the Classify button to manually classify it.</p> <p></p> <p>After clicking the Classify button, the predicted label will appear, thus showing the Language Intensity Classification of the comment. An example is shown below:</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#22-classifying-text-by-uploading-csv","title":"2.2 Classifying Text by Uploading CSV","text":"<p>In addition to single-comment classification, users can test multiple comments by uploading a CSV file via the Upload CSV tab. Simply click the Browse files button to select a file from your device.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#23-history-tab","title":"2.3 History Tab","text":"<p>Clicking into the History tab enables users to view their past classifications, allowing them to track and review their work.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#24-model-information-section","title":"2.4 Model Information Section","text":"<p>The Model Information section offers users detailed insights into the model powering the application, including its fine-tuning process and definitions for each of the 7 classification categories. This helps users understand how the model interprets and categorizes data, enabling more informed interpretation of outputs and confidence in the technology.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#3-analysis-page","title":"3. Analysis Page","text":"<p>The Analysis page presents a summary of our data analysis on hate and toxic speech in Singapore-related subreddits from 2020 to 2023. This analysis identifies trends in the frequency and intensity of such language, highlighting significant shifts over time and offering insights into the prevalence and nature of online hate speech and toxicity within the Singaporean context.</p>"},{"location":"home/technical_report/methodology/application/#31-overview-tab","title":"3.1 Overview Tab","text":"<p>The Overview tab welcomes users to the analysis dashboard and provides a brief guide to the various tabs and their functions.</p> <p></p>"},{"location":"home/technical_report/methodology/application/#32-upload-csv-tab","title":"3.2 Upload CSV Tab","text":"<p>This tab allows users to upload a CSV file with text data, formatted to meet the tool's input requirements. Key fields should include timestamp, comment text, and relevant metadata, such as user engagement or subreddit categorization. Once uploaded, the data is preprocessed and analyzed to generate metrics for frequency and intensity analyses.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#33-frequency-analysis-tab","title":"3.3 Frequency Analysis Tab","text":"<p>The Frequency Analysis tab displays the proportions of each of the 13 topics identified through topic modeling over selected years, offering insights into the distribution and prominence of topics and how their prevalence in hateful or toxic language has changed over time.</p> <p>To the right of the bar graph, users can customize the visualization with the following options:</p> <p>Year and Topic Selection: Users can select specific years or view data across all years and focus on particular topics or include all 13 topics.</p> <p>Color Palette Customisation: Adjust the color palette for easier distinction between topics and personalized display preferences.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#34-intensity-analysis-tab","title":"3.4 Intensity Analysis Tab","text":"<p>The Intensity Analysis section delves into how the intensity of toxic or hateful language has evolved, featuring multiple visualizations for detailed examination.</p>"},{"location":"home/technical_report/methodology/application/#341-net-trend-visualization","title":"3.4.1 Net Trend Visualization","text":"<p>This tab provides a graphical view of overall trends in hate and toxic speech intensity over time for the most frequent topics: Race, Crimes, and Gender. It highlights periods of increased or decreased intensity, offering a broader perspective on underlying trends.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#342-word-cloud","title":"3.4.2 Word Cloud","text":"<p>This tab presents a word cloud, visually displaying the most frequent subjects within the top three topics: Race, Crime, and Gender. Words appear in proportion to their frequency, emphasizing key subjects often associated with high levels of hatefulness and toxicity.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#343-filtered-comments","title":"3.4.3 Filtered Comments","text":"<p>This tab displays a list of selected comments within the top three topics: Race, Crimes, and Gender, categorized by hate and toxicity. Users can select from a dropdown to view comments in any of the six categories: Race Hate, Race Toxic, Crimes Hate, Crimes Toxic, Gender Hate, and Gender Toxic.</p> <p> </p>"},{"location":"home/technical_report/methodology/application/#35-findings-tab","title":"3.5 Findings Tab","text":"<p>The Findings tab summarizes key insights from the analysis, focusing on the top three topics\u2014Race, Crimes, and Gender\u2014which show heightened levels of hatefulness and toxicity. This section provides a detailed view of how these topics have been discussed in Singapore-related subreddits, highlighting trends and nuances within each topic's toxicity.</p> <p>Key features of this tab include:</p> <p>Personalized Filtering: Users can filter findings by comment type, and a keyword search function allows for pinpointing specific terms within the findings, facilitating targeted exploration of toxic or hateful language.</p> <p>Key Insights: etailed insights are provided across six categories: Race Hate, Race Toxic, Crimes Hate, Crimes Toxic, Gender Hate, and Gender Toxic, offering a deeper understanding of each category's context.</p> <p>These findings offer a comprehensive view of the hate and toxic speech landscape, illustrating sentiment evolution and identifying primary areas of concern.</p> <p> </p>"},{"location":"home/technical_report/methodology/data-processing/","title":"Data Processing","text":"<p>This subsection explains how we collected, cleaned, processed, used our data, and how we utilized Kedro to manage these tasks.</p>"},{"location":"home/technical_report/methodology/data-processing/#1-collection-and-cleaning","title":"1. Collection and Cleaning","text":"<p>Our data cleaning and processing involved merging two raw datasets from 2020-2023, totaling approximately 4.5 million Reddit comments. We removed rows with deleted, removed, or NaN values in the \"text\" column to retain only meaningful content. Next, we cleaned the text data by removing unwanted characters, extra spaces, and specific \"gt\" text artifacts which is a frequent result of extracting indented Reddit comments. We processed timestamps to extract the year, enabling temporal analysis. Additionally, we standardized the text by converting it to lowercase and removed common English stopwords, using NLTK\u2019s stopword corpus to ensure consistency. This pipeline ensured a clean, structured dataset for further classification and analysis.</p>"},{"location":"home/technical_report/methodology/data-processing/#11-kedro","title":"1.1 Kedro","text":"<p>Kedro provides a structured, reproducible framework for modular data workflows while keeping data private. Using Kedro pipelines, we established a clear sequence for data processing without embedding sensitive datasets in the codebase. This setup also ensures reproducibility and transparency, allowing the project to be retraced and modified without exposing data. The code of our pipeline can be found in here</p> <p> </p>   Figure 3. Visualisation of the pipelines in our Kedro project"},{"location":"home/technical_report/methodology/data-processing/#2-data-subsetting","title":"2. Data Subsetting","text":"<p>After calculating the mean and median word counts of the cleaned dataset (32 and 16 words, respectively), we shuffled the data and filtered for texts between 5 and 50 words, which retained 70% of the original dataset. From this filtered data, we then randomly selected a subset of 400,000 texts. Our code can be found here.</p>"},{"location":"home/technical_report/methodology/data-processing/#3-automated-data-labelling","title":"3. Automated Data Labelling","text":""},{"location":"home/technical_report/methodology/data-processing/#31-llm-selection","title":"3.1 LLM selection","text":"<p>We automatically labelled our subset of 400,000 comments using OpenAI's GPT-4o mini LLM, which outperformed other models like Mistral-7B-Instruct-v0.3 and Meta-Llama-3-8B-Instruct in aligning with our classification definitions.Using a custom-designed base prompt, we tested each of the three models on 11 example comments covering all 7 class definitions. The prompt was refined iteratively for each model to enhance classification accuracy across all examples. Final testing yielded scores of 10 for GPT-4o mini, 10 for Llama, and 8 for Mistral. Although both GPT-4o mini and Llama achieved high scores, we chose GPT-4o mini due to its more consistent and reliable performance across diverse examples, ensuring greater alignment with our classification criteria.</p> Text Mistral Llama3 GPT-4o mini Hate 1 teachers pushing their religious values b4 and go on a tangent and rant about LGBTQ please report them if you see that happening and feel safe enough to report. teachers doing so contravene national values such as religious harmony. Hate 1 Hate 2 Hate 1 Hate 2 I don't hire poor workers because they are all lazy and entitled. Hate 1 Hate 2 Hate 2 Toxic 1 Gen Z parents gotta stop giving their child ridiculous name. They are only gonna get bullied when they go to school. Toxic 1 Hate 1 Toxic 1 Toxic 2 Get the fuck outta here you bloody wumao. Toxic 2 Hate 2 Toxic 2 Toxic 3 Indeed these aren't discriminatory. Fuck off antivaxxers, get vaccinated or get fucked. Toxic 2 Toxic 3 Toxic 3   Table 1: Model Comparison on Selected Examples"},{"location":"home/technical_report/methodology/data-processing/#32-labelling-prompt","title":"3.2 Labelling prompt","text":"<p>Our final prompt can be found here. To optimize labelling accuracy with GPT-4o mini, we aimed to balance prompt length with effectiveness, incorporating several prompt engineering techniques that significantly improved classification outcomes:</p> <p>Chain-of-Thought Prompting: We structured the prompt in sequential steps to guide the model through sensitive group identification and then intensity classification. By explicitly separating each decision point (group sensitivity and language intensity), we helped the model follow a logical flow, reducing misclassification errors.</p> <p>Explicit Criteria Definition: We defined each class with clear, specific criteria, particularly for \"Hate\" and \"Toxic\" classifications. This removed ambiguity, especially in nuanced comments, and enabled the model to distinguish between different levels of intensity accurately.</p> <p>Few-Shot Prompting: Including labeled examples allowed GPT-4o mini to better grasp our custom classification standards. Each example was carefully selected to illustrate different class labels, giving the model concrete reference points.</p> <p>Instructional Constraints: We instructed the model to classify comments \"as they are,\" focusing only on context embedded within the comment itself and avoiding inferences that the model might generate independently.</p>"},{"location":"home/technical_report/methodology/data-processing/#33-dataset","title":"3.3 Dataset","text":"<p>We obtained the dataset with 400,000 texts labelled by GPT-4o mini. Table 2 shows the class distribution produced.</p> Classification Count No Hate/Toxic 355,079 Hate 1 22,641 Toxic 1 15,097 Toxic 2 5,563 Hate 2 967 Toxic 3 432 Hate 3 221   Table 2: Class distribution of 400,000 labelled data by GPT-4o mini   <p>We applied further preprocessing steps to this dataset before conducting Topic Modelling and training our Multiclass Text Classification Model.</p>"},{"location":"home/technical_report/methodology/modelling/model1/","title":"Multiclass Text Classification","text":""},{"location":"home/technical_report/methodology/modelling/model1/#overview","title":"Overview","text":"<p>We developed a custom multiclass text classification model tailored to encapsulate our predefined definitions of hate and toxic speech. This approach not only ensures alignment with our specific criteria but also enhances scalability by eliminating the future need to rely on external API calls for labeling, allowing us to handle datasets independently.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#1-data-generation","title":"1. Data generation","text":"<p>The 400,000 labeled comments revealed a significant class imbalance (Table 2) with Hate 2, Hate 3, Toxic 2, and Toxic 3 having significantly lower counts. Initially, we addressed class imbalance by applying calculated class weights to <code>CrossEntropyLoss</code> and using a stratified split for balanced training, validation, and test sets. However, DistilBERT still struggled with minority classes, so we generated synthetic data to create a fully balanced dataset, eliminating the need for class weights and significantly improving model performance.</p> <p>Standard techniques like Synthetic Minority Over-sampling TEchnique <code>(SMOTE)</code>, undersampling, and oversampling with <code>imblearn</code> were applied but ineffective, as identifying hate and toxic speech in Reddit text often requires nuanced attention that these methods do not capture well. To address this, we opted to synthetically generate data for classes with fewer than 15,000 examples and randomly sampled 15,000 instances from larger classes. When we tried with a lower threshold of 5000 per class, we observed that across all models, performance was lower. We chose the 15,000 threshold ultimately to minimize the number of classes needing synthetic data, ensuring balanced representation with minimal generation. Our template-based data generation approach drew inspiration from SGHateCheck.</p> <p>We used a template-based approach to generate data for four classes\u2014Hate 2, Hate 3, Toxic 2, and Toxic 3\u2014tailoring each template to fit the tone and language of each class. \"Hate\" templates included sensitive groups and biased actions, while \"Toxic\" templates reflected general disrespect without targeting sensitive groups. Combining templates with action and group lists through a Cartesian product, we created diverse, contextually relevant synthetic comments aligned with our classification definitions, enhancing model training for underrepresented classes. Although fixed templates limit flexibility, generating examples for multiclass definitions allow us to capture various forms and intensities of hate effectively.</p> <p></p>   Figure 4. Sample generation of Hate 3 and Toxic 2 example   <p>The final dataset consisted of 105,000 labeled texts, with a balanced distribution of 15,000 texts for each of the 7 classes. It consists of the data labeled by GPT-4o mini and data that was synthetically generated using our templates. The dataset was split into train (70%), validation (15%), and test (15%) for us to train and evaluate Distilbert and other traditional machine learning models.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#2-distilbert-model","title":"2. DistilBERT Model","text":"<p>To leverage transformer-based architecture without excessive computational cost, we selected DistilBERT, a distilled version of BERT, which provides a balance of efficiency and accuracy by retaining the core transformer architecture in a more lightweight model. DistilBERT can capture nuanced linguistic features essential for hate and toxic speech classification. The codes can be found here.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#21-data-preparation","title":"2.1 Data Preparation","text":"<p>The DistilBERT tokenizer, specifically the distilbert-base-multilingual-cased variant, captures word context and relationships, crucial for nuanced hate and toxic speech detection. It uses subword tokenization, enabling the model to handle slang, out-of-vocabulary words, and multilingual contexts commonly found in online text. We tokenized each text sample with this multilingual tokenizer, setting a maximum token length of 128 to balance context preservation and memory efficiency. Labels were mapped to integer values across seven predefined classes, and data loaders were configured with a batch size of 128 for efficient model training and evaluation.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#22-training-and-hyperparameter-tuning","title":"2.2 Training and Hyperparameter Tuning","text":"<p>DistilBERT was trained over three epochs with a learning rate of 1e-5. Dropout rates were set to 0.2 for both attention and hidden layers to reduce overfitting. The AdamW optimizer with a weight decay of 0.01 was used, and a learning rate scheduler with warmup steps (10% of total training steps) was implemented to gradually adjust the learning rate during training.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#23-training-process","title":"2.3 Training Process","text":"<p>During each epoch, the model computed loss and accuracy across batches, with scheduler steps adjusting the learning rate. After each epoch, validation performance was evaluated, and the model with the best validation loss was saved for testing.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#24-evaluation-and-testing","title":"2.4 Evaluation and Testing","text":"<p>The best-performing DistilBERT model was then evaluated on the test set, using accuracy, loss, and weighted F1 score as metrics. F1 score was particularly important to balance precision and recall across all classes, given the inherent imbalance.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#3-traditional-machine-learning-models","title":"3. Traditional Machine Learning Models","text":"<p>To benchmark performance, we also employed traditional machine learning algorithms on the dataset. The codes can be found here.</p> <ol> <li> <p>Ridge Regression: Tuned with 5-fold cross-validation using <code>KFold</code>, achieving a best alpha of 4.0.</p> </li> <li> <p>XGBoost: An efficient and scalable implementation of gradient-boosted decision trees. Optimized with a <code>grid search</code>, with best parameters of learning_rate=0.1, max_depth=5, n_estimators=100, and subsample=0.5.</p> </li> <li> <p>Multinomial Naive Bayes: Used as a baseline due to computational efficiency. The algorithm is based on the Bayes theorem and is widely used for multiclass classification.</p> </li> <li> <p>Linear Support Vector Machine: Included for its performance in high-dimensional spaces but with minimal tuning.</p> </li> </ol>"},{"location":"home/technical_report/methodology/modelling/model1/#31-preprocessing-for-traditional-models","title":"3.1 Preprocessing for Traditional Models","text":"<p>We applied tokenization, stopword removal, lemmatization, and Term Frequency-Inverse Document Frequency (<code>TF-IDF</code>) vectorization (max 5000 features focused on unigrams) to convert text into numerical representations for traditional models. TF-IDF was chosen for its efficiency in creating sparse, interpretable feature vectors that work well with linear classifiers like Ridge and Naive Bayes. Although TF-IDF lacks contextual depth, it remains effective in capturing term importance and distinguishing words across classes, offering a suitable baseline for simpler models. Labels were mapped to integers, and the dataset was split into training, validation, and test sets using the same 70-15-15 ratio and balanced distribution applied for DistilBERT.</p>"},{"location":"home/technical_report/methodology/modelling/model1/#4-evaluation","title":"4. Evaluation","text":"<p>For all models, we used accuracy, loss, and macro-average F1 score as evaluation metrics. The F1 score, which balances precision and recall, was critical in this project due to the multiclass nature of our task, where misclassifications across classes (even if balanced) can have different consequences. Since we had balanced the classes prior to training, we found macro-average F1-scores appropriate.</p>"},{"location":"home/technical_report/methodology/modelling/model2/","title":"Topic Modelling","text":""},{"location":"home/technical_report/methodology/modelling/model2/#1-overview","title":"1. Overview","text":"<p>This analysis aims to determine if and why the topics and comments on Reddit have become more hateful and toxic over recent years. To conduct this investigation, we employed a two-pronged analytical approach focusing on the frequency and intensity of toxic and hateful comments. The codes used to derive our analysis can be found here.</p>"},{"location":"home/technical_report/methodology/modelling/model2/#2-data-processing-and-topic-modeling","title":"2. Data Processing and Topic Modeling","text":"<p>The primary dataset utilized consisted of comments extracted from Reddit, with text data located in the <code>text</code> column of our DataFrame. We initiated our analysis by applying a topic modeling technique using BERTopic to categorize the comments into various topics.</p> <p></p>   Figure 5. Topic Modelling Pipeline"},{"location":"home/technical_report/methodology/modelling/model2/#21-text-preprocessing-and-normalization","title":"2.1. Text Preprocessing and Normalization","text":"<p>To address the linguistic characteristics of the Reddit dataset, which includes colloquial and region-specific language known as Singlish, we implemented a preprocessing routine tailored to convert Singlish terms into standard English. This preprocessing step involved the following actions:</p> <ol> <li> <p>Dictionary Mapping: A custom dictionary was developed to translate commonly used Singlish expressions to their English equivalents, which included removing expletives and colloquialisms that could skew the analysis. This dictionary also helped standardize different spellings of similar terms (e.g., \u201cgahmen\u201d and \u201cgahment\u201d both mapped to \u201cgovernment\u201d).</p> </li> <li> <p>Text Normalization: The text data was converted to lowercase to maintain consistency in processing. Special characters were removed to clean the text of any punctuation or extraneous symbols that do not contribute to semantic meaning.</p> </li> <li> <p>Stop Words Removal: We integrated NLTK\u2019s list of English stop words with additional custom stop words specific to our dataset to filter out noise and focus on meaningful words.</p> </li> <li> <p>Text Cleaning Function: A text cleaning function was developed to automate the normalization process, which involved tokenization, dictionary-based term replacement, and stop word removal. This function was applied to each text entry in the dataset.</p> </li> </ol>"},{"location":"home/technical_report/methodology/modelling/model2/#22-setting-up-bertopic-for-topic-modeling","title":"2.2. Setting Up BERTopic for Topic Modeling","text":"<p>After preprocessing, we utilized BERTopic, an advanced topic modeling technique that leverages state-of-the-art language models and machine learning algorithms to discover topics within text data. The BERTopic setup involved several components designed to optimize topic extraction:</p> <ol> <li>Sentence Transformer: The <code>SentenceTransformer</code> model, specifically <code>distiluse-base-multilingual-cased-v1</code>, was employed to generate dense vector representations of the cleaned text, capturing the contextual relationships between words.</li> <li>Dimensionality Reduction: UMAP was configured to reduce the high-dimensional space of text embeddings into a lower-dimensional space that preserves the most important structural aspects of the data, facilitating more effective clustering.</li> <li>Clustering: HDBSCAN was used to perform density-based clustering on the reduced embeddings, identifying groups of text with similar content without requiring a predetermined number of clusters.</li> <li>Representation Model: A <code>KeyBERTInspired</code> representation model was implemented to select the most representative words for each topic based on their relevance and frequency, providing an interpretable summary of each topic.</li> </ol> <p>During the topic modeling process, we identified a significant number of comments labeled with the topic identifier <code>-1</code>, indicating classification as outliers.</p> <p>Upon further investigation of these outlier comments, we observed that these comments were not assigned appropriate topics during the initial topic modeling phase even though the comments were relevant. To rectify this, we opted to reassess the <code>-1</code> labeled comments by extracting the Reddit thread topic from the <code>linkid</code> column rather than relying solely on the text content. This approach allowed us to understand the context and nuances surrounding the comments, which may lack explicit thematic elements yet still contribute to the overall discourse.</p>"},{"location":"home/technical_report/methodology/modelling/model2/#3-refinement-of-topic-modeling","title":"3. Refinement of Topic Modeling","text":"<p>After re-evaluating the outlier comments, we conducted a secondary BERTopic modeling on the reddit thread topic. The refined topics were then merged with the previously generated topics, forming a unified dataframe that ensured comprehensive coverage and minimized information loss.</p> <p>Subsequently, we categorized these topics into twelve main themes for detailed analysis:</p> # Topic 1 Body Image 2 COVID-19 3 Crimes 4 Education 5 Gender 6 Generational 7 Government 8 Housing 9 LGBTQ+ 10 Religion 11 Transportation 12 Work 13 Race <p>We leveraged BERTopic\u2019s merging capabilities to consolidate related sub-topics under these main themes. Topics that did not align with these categories were classified as null, maintaining clarity and focus in thematic analysis.</p>"},{"location":"home/technical_report/methodology/modelling/model2/#4-analysis-of-hate-toxic-frequency-for-topics","title":"4. Analysis of Hate &amp; Toxic Frequency for Topics","text":"<p>Following the thematic categorization, we quantified the frequency of comments per topic to identify the most prevalent discussions. The frequency analysis aimed to pinpoint the top 3 topics that dominate the platform with regard to hate and toxicity and assess their evolution over time.</p> <p></p>   Figure 6. Frequency of hate and toxic for each topic for all years   <p></p>   Figure 7. Frequency of hate and toxic for each topic across each years"},{"location":"home/technical_report/methodology/modelling/model2/#5-analysis-of-hate-and-toxic-intensity-for-topics","title":"5. Analysis of Hate and Toxic Intensity for Topics","text":"<p>Thereafter, we dived into the respective hate and toxicity intensity trends for these 3 dominant topics from 2020 to 2023. This analysis examines hate and toxicity intensity trends across key topics by calculating trends, visualizing changes, filtering significant years, and generating insights through word clouds and problem statements.</p> <p></p>   Figure 8. Methodology for Analysis for a Topic   <p>Our methodology involves several components to generate insights:</p>"},{"location":"home/technical_report/methodology/modelling/model2/#51-trend-calculation","title":"5.1. Trend Calculation","text":"<p>By examining the trends, we aim to provide a comprehensive view of hate and toxicity shifts over time.</p> <ol> <li>Calculate Indices for each Intensity Level</li> </ol> <p>Firstly, we calculate the indices for hate and toxicity levels based on Reddit comment data. The indices are determined by analyzing the proportion of comments at each intensity level (e.g., Hate 1, Hate 2, Hate 3, Toxic 1, Toxic 2, Toxic 3) for each topic on a yearly basis. This provides a relative measure of the distribution of intensity levels over time.</p> <ul> <li>Formula:</li> </ul> <p></p> <ol> <li>Find Yearly Change of Index at each Intensity Level</li> </ol> <p>We then find out if the proportion of these indices have decreased or increased from the previous year by calculating the proportion change of indices at each intensity level.</p> <ul> <li>Formula:</li> </ul> <p></p> <ol> <li>Aggregating Yearly Change in Hate and Toxic Intensities</li> </ol> <p>For a cumulative view of whether total hate and toxicity levels have increased or decreased over time, a weighted sum formula is used. This formula sums the yearly changes in hate and toxicity indices, with higher weights given to more severe levels. This helps us determine if overall hate or toxicity levels increase in that year.</p> <ul> <li>Formula:</li> </ul> <p></p> <p>Net trends can also be visualized to track changes more easily.</p> <p></p>   Figure 9. Example of Net Hate Trend Visualisation"},{"location":"home/technical_report/methodology/modelling/model2/#52-positive-trend-filtering-and-comment-extraction","title":"5.2. Positive Trend Filtering and Comment Extraction","text":"<p>Next, we focus the analysis on years that show a positive increase in the net trend for hate and toxic comments, helping to isolate periods of growing concern.</p>"},{"location":"home/technical_report/methodology/modelling/model2/#53-subtopic-examination-filtering-and-sequencing-of-comments","title":"5.3. Subtopic Examination, Filtering and Sequencing of Comments","text":"<ol> <li> <p>Visualize Common Subtopics: Generate word clouds to highlight the most frequent subtopics within each main topic, using comments from years with positive trend increases (Figure 10).</p> </li> <li> <p>Filter Comments by Top Subtopic: Filter comments to focus on the top subtopic within each main topic, providing a more granular view of trending issues.</p> </li> <li> <p>Enhance Comments with Contextual Titles: Append relevant Reddit titles to each comment to add context to comments.</p> </li> <li> <p>Rank and Sequence High-Intensity Comments: Rank comments by intensity and select the top 20 for each subtopic, enabling an in-depth examination of high-intensity sentiments within the subtopic.</p> </li> </ol> <p></p>   Figure 10. Word Cloud for Hate in Race Topic, highlighting subtopics such as 'china' and 'foreigner'."},{"location":"home/technical_report/methodology/modelling/model2/#54-problem-statement-generation","title":"5.4. Problem Statement Generation","text":"<p>Finally, we formulated problem statements that encapsulate the main issues within each topic with the filtered comments of highest intensity levels.</p> <p>To uncover greater insights of the issues, we did literature reviews and explored possible solutions upon further examination of the problem statements. Our findings can be found here.</p>"}]}