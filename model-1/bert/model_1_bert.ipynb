{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load on GPU\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the given CSV file into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(filepath, engine=\"python\")\n",
    "        print(f\"Data successfully loaded from {filepath}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reading the CSV file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from C:\\Users\\richm\\OneDrive\\Desktop\\DSA4264\\DSA4264-Detoxify\\clean_concatenated_texts_data_with_labels.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>link</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>moderation</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 82</th>\n",
       "      <th>Unnamed: 83</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>Unnamed: 85</th>\n",
       "      <th>Unnamed: 86</th>\n",
       "      <th>Unnamed: 87</th>\n",
       "      <th>Unnamed: 88</th>\n",
       "      <th>Unnamed: 89</th>\n",
       "      <th>Unnamed: 90</th>\n",
       "      <th>Unnamed: 91</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id of j2gwp5d said this: The best.</td>\n",
       "      <td>1/1/2023 4:51</td>\n",
       "      <td>MisoMesoMilo</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t1_j2gje0f</td>\n",
       "      <td>j2gwp5d</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id of j2i2mtw said this: Wah heart pain.\\nid o...</td>\n",
       "      <td>1/1/2023 14:03</td>\n",
       "      <td>MisoMesoMilo</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t1_j2hyfm2</td>\n",
       "      <td>j2i2mtw</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>id of j2h2alh said this: HNY DT where to go to...</td>\n",
       "      <td>1/1/2023 5:48</td>\n",
       "      <td>dazark</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>j2h2alh</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>id of j2hpea5 said this: Clubbing shenanigans,...</td>\n",
       "      <td>1/1/2023 11:08</td>\n",
       "      <td>N1_Procrastinator</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t1_j2ho0m4</td>\n",
       "      <td>j2hpea5</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id of j2hewei said this: Oooff...was supposed ...</td>\n",
       "      <td>1/1/2023 8:28</td>\n",
       "      <td>EaeleButEeelier</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>j2hewei</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>1048555</td>\n",
       "      <td>id of fw5t8zt said this: Tharman will hard car...</td>\n",
       "      <td>6/27/2020 12:32</td>\n",
       "      <td>wank_for_peace</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw55rd0</td>\n",
       "      <td>fw5t8zt</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>1048556</td>\n",
       "      <td>id of fw5djn0 said this: Ivan whole day long p...</td>\n",
       "      <td>6/27/2020 8:16</td>\n",
       "      <td>show-up</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw52891</td>\n",
       "      <td>fw5djn0</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>1048557</td>\n",
       "      <td>id of fw5buyl said this: He is saying people a...</td>\n",
       "      <td>6/27/2020 7:47</td>\n",
       "      <td>CanISmellYourPanty</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw56nrv</td>\n",
       "      <td>fw5buyl</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>1048558</td>\n",
       "      <td>id of fw54fmr said this: Eh you want say can s...</td>\n",
       "      <td>6/27/2020 5:52</td>\n",
       "      <td>thrulim123</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw52891</td>\n",
       "      <td>fw54fmr</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>1048559</td>\n",
       "      <td>id of fw558gh said this: Dont need ah, he alre...</td>\n",
       "      <td>6/27/2020 6:03</td>\n",
       "      <td>freakshow504</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw53rc3</td>\n",
       "      <td>fw558gh</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "0                0                 id of j2gwp5d said this: The best.   \n",
       "1                1  id of j2i2mtw said this: Wah heart pain.\\nid o...   \n",
       "2                2  id of j2h2alh said this: HNY DT where to go to...   \n",
       "3                3  id of j2hpea5 said this: Clubbing shenanigans,...   \n",
       "4                4  id of j2hewei said this: Oooff...was supposed ...   \n",
       "...            ...                                                ...   \n",
       "1048570    1048555  id of fw5t8zt said this: Tharman will hard car...   \n",
       "1048571    1048556  id of fw5djn0 said this: Ivan whole day long p...   \n",
       "1048572    1048557  id of fw5buyl said this: He is saying people a...   \n",
       "1048573    1048558  id of fw54fmr said this: Eh you want say can s...   \n",
       "1048574    1048559  id of fw558gh said this: Dont need ah, he alre...   \n",
       "\n",
       "               timestamp            username  \\\n",
       "0          1/1/2023 4:51        MisoMesoMilo   \n",
       "1         1/1/2023 14:03        MisoMesoMilo   \n",
       "2          1/1/2023 5:48              dazark   \n",
       "3         1/1/2023 11:08   N1_Procrastinator   \n",
       "4          1/1/2023 8:28     EaeleButEeelier   \n",
       "...                  ...                 ...   \n",
       "1048570  6/27/2020 12:32      wank_for_peace   \n",
       "1048571   6/27/2020 8:16             show-up   \n",
       "1048572   6/27/2020 7:47  CanISmellYourPanty   \n",
       "1048573   6/27/2020 5:52          thrulim123   \n",
       "1048574   6/27/2020 6:03        freakshow504   \n",
       "\n",
       "                                                      link     link_id  \\\n",
       "0        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "1        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "2        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "3        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "4        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "...                                                    ...         ...   \n",
       "1048570  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048571  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048572  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048573  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048574  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "\n",
       "          parent_id       id subreddit_id  \\\n",
       "0        t1_j2gje0f  j2gwp5d     t5_2qh8c   \n",
       "1        t1_j2hyfm2  j2i2mtw     t5_2qh8c   \n",
       "2        t3_1004s1o  j2h2alh     t5_2qh8c   \n",
       "3        t1_j2ho0m4  j2hpea5     t5_2qh8c   \n",
       "4        t3_1004s1o  j2hewei     t5_2qh8c   \n",
       "...             ...      ...          ...   \n",
       "1048570  t1_fw55rd0  fw5t8zt     t5_2qh8c   \n",
       "1048571  t1_fw52891  fw5djn0     t5_2qh8c   \n",
       "1048572  t1_fw56nrv  fw5buyl     t5_2qh8c   \n",
       "1048573  t1_fw52891  fw54fmr     t5_2qh8c   \n",
       "1048574  t1_fw53rc3  fw558gh     t5_2qh8c   \n",
       "\n",
       "                                                moderation  ... Unnamed: 82  \\\n",
       "0        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "1        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "2        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "3        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "4        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "...                                                    ...  ...         ...   \n",
       "1048570  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048571  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048572  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048573  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048574  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "\n",
       "        Unnamed: 83 Unnamed: 84 Unnamed: 85 Unnamed: 86 Unnamed: 87  \\\n",
       "0               NaN         NaN         NaN         NaN         NaN   \n",
       "1               NaN         NaN         NaN         NaN         NaN   \n",
       "2               NaN         NaN         NaN         NaN         NaN   \n",
       "3               NaN         NaN         NaN         NaN         NaN   \n",
       "4               NaN         NaN         NaN         NaN         NaN   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1048570         NaN         NaN         NaN         NaN         NaN   \n",
       "1048571         NaN         NaN         NaN         NaN         NaN   \n",
       "1048572         NaN         NaN         NaN         NaN         NaN   \n",
       "1048573         NaN         NaN         NaN         NaN         NaN   \n",
       "1048574         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "        Unnamed: 88 Unnamed: 89 Unnamed: 90 Unnamed: 91  \n",
       "0               NaN         NaN         NaN         NaN  \n",
       "1               NaN         NaN         NaN         NaN  \n",
       "2               NaN         NaN         NaN         NaN  \n",
       "3               NaN         NaN         NaN         NaN  \n",
       "4               NaN         NaN         NaN         NaN  \n",
       "...             ...         ...         ...         ...  \n",
       "1048570         NaN         NaN         NaN         NaN  \n",
       "1048571         NaN         NaN         NaN         NaN  \n",
       "1048572         NaN         NaN         NaN         NaN  \n",
       "1048573         NaN         NaN         NaN         NaN  \n",
       "1048574         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[1048575 rows x 92 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_csv(\n",
    "    r\"C:\\Users\\richm\\OneDrive\\Desktop\\DSA4264\\DSA4264-Detoxify\\balanced_data.csv\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model training.\n",
    "MAX_LEN = 200\n",
    "num_classes = 7\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VALID_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update class-to-id mapping for 7 classes\n",
    "cls_to_id = {\n",
    "    \"No Hate/Toxic\": 0,\n",
    "    \"Hate 1\": 1,\n",
    "    \"Hate 2\": 2,\n",
    "    \"Hate 3\": 3,\n",
    "    \"Toxic 1\": 4,\n",
    "    \"Toxic 2\": 5,\n",
    "    \"Toxic 3\": 6,\n",
    "}\n",
    "\n",
    "id_to_cls = {v: k for k, v in cls_to_id.items()}\n",
    "\n",
    "num_classes = 7  # Update the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richm\\AppData\\Local\\Temp\\ipykernel_23920\\1372939726.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['gold_label'] = filtered_df['gold_label'].map(cls_to_id)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integer format\n",
    "data[\"gold_label\"] = data[\"gold_label\"].map(cls_to_id)\n",
    "data = data.dropna(subset=[\"gold_label\"])\n",
    "data[\"gold_label\"] = data[\"gold_label\"].astype(int)\n",
    "\n",
    "# Step 1: Split the data into 70% train and 30% (validation + test)\n",
    "df_train, df_temp = train_test_split(\n",
    "    data,\n",
    "    test_size=0.3,  # 30% for validation and test\n",
    "    shuffle=True,  # Shuffle only during the first split\n",
    "    stratify=data[\"gold_label\"],  # Maintain class distribution\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Step 2: Split the remaining 30% into 15% validation and 15% test\n",
    "# NOTE: Now stratifying using the labels from df_temp, not data\n",
    "df_valid, df_test = train_test_split(\n",
    "    df_temp,\n",
    "    test_size=0.5,  # Split 50-50 between validation and test from the remaining 30%\n",
    "    shuffle=True,\n",
    "    stratify=df_temp[\"gold_label\"],  # Correct stratification\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Display the sizes of each set\n",
    "print(f\"Training set size: {len(df_train)}\")\n",
    "print(f\"Validation set size: {len(df_valid)}\")\n",
    "print(f\"Test set size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.targets = df[\"gold_label\"].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        text = \" \".join(text.split())  # Clean text\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": inputs[\"token_type_ids\"].flatten(),\n",
    "            \"targets\": torch.tensor(self.targets[index], dtype=torch.long),\n",
    "            \"text\": text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have 10,000 samples per class and 7 classes\n",
    "num_samples = len(df_train)\n",
    "num_classes = len(np.unique(df_train[\"gold_label\"]))\n",
    "class_counts = np.bincount(df_train[\"gold_label\"])\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights (inverse of class frequency)\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = class_weights[df_train[\"gold_label\"]]\n",
    "print(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WeightedRandomSampler for balanced class sampling for the training dataset\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,  # You need to define how to calculate these sample weights\n",
    "    num_samples=num_samples,\n",
    "    replacement=False,\n",
    ")\n",
    "\n",
    "# Create datasets for train, validation, and test sets\n",
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)  # Add the test dataset\n",
    "\n",
    "# Data loaders\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,  # Use the weighted sampler here\n",
    "    num_workers=4,  # Increase num_workers for faster loading\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle validation data\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,  # Typically use the same batch size as validation\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Check the length of the datasets\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")  # Check the size of the test dataset\n",
    "\n",
    "# Check the number of batches in the DataLoader\n",
    "print(f\"Number of batches in training DataLoader: {len(train_data_loader)}\")\n",
    "print(f\"Number of batches in validation DataLoader: {len(val_data_loader)}\")\n",
    "print(\n",
    "    f\"Number of batches in test DataLoader: {len(test_data_loader)}\"\n",
    ")  # Check the number of batches for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", return_dict=True\n",
    "        )\n",
    "\n",
    "        # Additional fully connected and dropout layers\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Final output layer for classification\n",
    "        self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        x = self.fc1(output.pooler_output)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights based on the balanced data\n",
    "class_counts = data[\"gold_label\"].value_counts().sort_index()\n",
    "class_weights = 1.0 / class_counts  # Inverse of class frequency\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize the weights\n",
    "\n",
    "# Convert to a tensor and move to the correct device (GPU)\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float).to(device)\n",
    "\n",
    "# Use class weights in CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/900 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = BERTClass(num_classes=num_classes)\n",
    "\n",
    "# Move the model to the appropriate device (use all available GPUs)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function (already set with class weights) and optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()  # Record the start time of the epoch\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Create a progress bar for the training data loader\n",
    "    train_loader_tqdm = tqdm(\n",
    "        train_data_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Training step with progress bar\n",
    "    for batch in train_loader_tqdm:\n",
    "        ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        labels = batch[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(ids, mask, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Collect labels and predictions for precision, recall, and F1 calculation\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        # Update the progress bar with loss\n",
    "        train_loader_tqdm.set_postfix(\n",
    "            {\"train_loss\": total_loss / (train_loader_tqdm.n + 1)}\n",
    "        )\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    accuracy = correct_predictions / len(train_data_loader.dataset)\n",
    "\n",
    "    # Calculate precision, recall, and F1 for training\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    # Record the end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time  # Time taken for this epoch\n",
    "\n",
    "    print(f\"Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "    print(\n",
    "        f\"Train Precision: {precision:.4f}, Train Recall: {recall:.4f}, Train F1-Score: {f1:.4f}\"\n",
    "    )\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    # Validation step with progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    # Create a progress bar for the validation data loader\n",
    "    val_loader_tqdm = tqdm(val_data_loader, desc=\"Validating\", unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_tqdm:\n",
    "            ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            labels = batch[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "            logits = model(ids, mask, token_type_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            val_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            # Collect validation labels and predictions for metrics\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Update the progress bar with validation loss\n",
    "            val_loader_tqdm.set_postfix(\n",
    "                {\"val_loss\": val_loss / (val_loader_tqdm.n + 1)}\n",
    "            )\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_data_loader)\n",
    "    val_accuracy = val_correct / len(val_data_loader.dataset)\n",
    "\n",
    "    # Calculate precision, recall, and F1 for validation\n",
    "    val_precision = precision_score(val_labels, val_preds, average=\"weighted\")\n",
    "    val_recall = recall_score(val_labels, val_preds, average=\"weighted\")\n",
    "    val_f1 = f1_score(val_labels, val_preds, average=\"weighted\")\n",
    "\n",
    "    print(\n",
    "        f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1-Score: {val_f1:.4f}\"\n",
    "    )\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_number = 4\n",
    "\n",
    "# Save the trained model weights\n",
    "model_save_path = f\"bert_model_{training_number}.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Initialize the model architecture (exactly the same as when the model was trained)\n",
    "model = BERTClass(num_classes=num_classes)\n",
    "\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the saved model weights onto the correct device\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a validation DataLoader\n",
    "\n",
    "\n",
    "def get_predictions_and_labels(model, data_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    texts = []\n",
    "    probabilities = []\n",
    "\n",
    "    # Wrap the data loader with tqdm to create a progress bar\n",
    "    loader_tqdm = tqdm(data_loader, desc=\"Generating predictions\", unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_tqdm:  # Use tqdm wrapped data_loader\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"targets\"].to(device)\n",
    "            text_batch = batch[\"text\"]  # Collecting texts\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            texts.extend(text_batch)  # Collect texts\n",
    "            probabilities.extend(probs.cpu().numpy())  # Store probabilities\n",
    "\n",
    "    return np.array(predictions), np.array(true_labels), texts, np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and labels for the test set\n",
    "predictions, true_labels, texts, probabilities = get_predictions_and_labels(\n",
    "    model, test_data_loader, device\n",
    ")\n",
    "# Extract class names from id_to_cls mapping\n",
    "class_names = [id_to_cls[i] for i in range(num_classes)]\n",
    "print(classification_report(true_labels, predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class-wise accuracy with tqdm\n",
    "\n",
    "\n",
    "def calculate_class_wise_accuracy(conf_matrix):\n",
    "    # True Positives for each class are the diagonal elements\n",
    "    true_positives = np.diag(conf_matrix)\n",
    "\n",
    "    # Support (Total actual instances for each class)\n",
    "    support = conf_matrix.sum(axis=1)\n",
    "\n",
    "    # Initialize list to store class-wise accuracy\n",
    "    class_wise_accuracy = []\n",
    "\n",
    "    # Use tqdm to show progress while calculating accuracy for each class\n",
    "    for i in tqdm(\n",
    "        range(len(true_positives)), desc=\"Calculating class-wise accuracy\", unit=\"class\"\n",
    "    ):\n",
    "        if support[i] != 0:  # Avoid division by zero\n",
    "            accuracy = true_positives[i] / support[i]\n",
    "        else:\n",
    "            accuracy = 0.0\n",
    "        class_wise_accuracy.append(accuracy)\n",
    "\n",
    "    return np.array(class_wise_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get misclassified samples with tqdm\n",
    "\n",
    "\n",
    "def get_misclassified_samples(predictions, true_labels, texts):\n",
    "    misclassified = []\n",
    "\n",
    "    # Wrap the iteration with tqdm for a progress bar\n",
    "    for pred, true, text in tqdm(\n",
    "        zip(predictions, true_labels, texts),\n",
    "        desc=\"Finding misclassified samples\",\n",
    "        total=len(predictions),\n",
    "        unit=\"sample\",\n",
    "    ):\n",
    "        if pred != true:\n",
    "            misclassified.append(\n",
    "                {\"text\": text, \"true_label\": true, \"predicted_label\": pred}\n",
    "            )\n",
    "\n",
    "    return misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, and F1-score (with zero_division handling)\n",
    "precision = precision_score(\n",
    "    true_labels, predictions, average=\"weighted\", zero_division=0\n",
    ")\n",
    "recall = recall_score(true_labels, predictions, average=\"weighted\", zero_division=0)\n",
    "f1 = f1_score(true_labels, predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Weighted): {precision:.4f}\")\n",
    "print(f\"Recall (Weighted): {recall:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {f1:.4f}\")\n",
    "\n",
    "# Normalize confusion matrix\n",
    "conf_matrix_normalized = (\n",
    "    conf_matrix.astype(\"float\") / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    ")\n",
    "\n",
    "# Set plot size and style\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.set(font_scale=1.2)  # Adjust font size\n",
    "\n",
    "# Plot normalized confusion matrix with percentage formatting\n",
    "ax = sns.heatmap(\n",
    "    conf_matrix_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=list(cls_to_id.keys()),\n",
    "    yticklabels=list(cls_to_id.keys()),\n",
    ")\n",
    "\n",
    "# Ensure equal aspect ratio for x and y axes\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# Improve plot aesthetics\n",
    "ax.set_xlabel(\"Predicted Labels\", fontsize=14)\n",
    "ax.set_ylabel(\"True Labels\", fontsize=14)\n",
    "ax.set_title(\"Normalized Confusion Matrix\", fontsize=16)\n",
    "\n",
    "# Adjust tick positions and spread them outwards\n",
    "ax.xaxis.set_ticks_position(\"top\")  # Move x-axis ticks to the top\n",
    "ax.xaxis.set_label_position(\"top\")  # Move x-axis label to the top\n",
    "\n",
    "# Set xticks and yticks to center-align with matrix cells\n",
    "ax.set_xticks([i + 0.5 for i in range(len(cls_to_id))])\n",
    "ax.set_yticks([i + 0.5 for i in range(len(cls_to_id))])\n",
    "\n",
    "# Apply label rotation to improve readability (optional)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(\"normalized_confusion_matrix.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified samples\n",
    "misclassified_samples = get_misclassified_samples(predictions, true_labels, texts)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "misclassified_df = pd.DataFrame(misclassified_samples)\n",
    "\n",
    "misclassified_df[\"true_label\"] = misclassified_df[\"true_label\"].apply(\n",
    "    lambda x: id_to_cls[x]\n",
    ")\n",
    "misclassified_df[\"predicted_label\"] = misclassified_df[\"predicted_label\"].apply(\n",
    "    lambda x: id_to_cls[x]\n",
    ")\n",
    "\n",
    "# Display misclassified samples\n",
    "print(misclassified_df.shape, len(predictions))\n",
    "# misclassified_df.to_csv('./data/misclassified_df.csv', index=False)\n",
    "misclassified_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
