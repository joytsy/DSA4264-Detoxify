{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.targets = df[\"Classification\"].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        target = self.targets[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"targets\": torch.tensor(target, dtype=torch.long),  # Ensure correct dtype\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert String Labels to Integers\n",
    "label_mapping = {\n",
    "    \"No Hate/Toxic\": 0,\n",
    "    \"Toxic 1\": 1,\n",
    "    \"Toxic 2\": 2,\n",
    "    \"Toxic 3\": 3,\n",
    "    \"Hate 1\": 4,\n",
    "    \"Hate 2\": 5,\n",
    "    \"Hate 3\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"training_data_15k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map string labels to integers\n",
    "df[\"Classification\"] = df[\"Classification\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      " Classification\n",
      "3    3500\n",
      "2    3500\n",
      "1    3500\n",
      "0    3500\n",
      "5    3500\n",
      "6    3500\n",
      "4    3500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set distribution:\n",
      " Classification\n",
      "4    750\n",
      "6    750\n",
      "3    750\n",
      "5    750\n",
      "2    750\n",
      "1    750\n",
      "0    750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      " Classification\n",
      "5    750\n",
      "6    750\n",
      "4    750\n",
      "1    750\n",
      "3    750\n",
      "2    750\n",
      "0    750\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratified train-test split to maintain class balance\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, random_state=42, stratify=df[\"Classification\"]\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"Classification\"]\n",
    ")\n",
    "\n",
    "# Check the class distribution in each split\n",
    "print(\"Training set distribution:\\n\", train_df[\"Classification\"].value_counts())\n",
    "print(\"\\nValidation set distribution:\\n\", val_df[\"Classification\"].value_counts())\n",
    "print(\"\\nTest set distribution:\\n\", test_df[\"Classification\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_CLASSES = 7  # For multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=train_df[\"Classification\"].unique(),\n",
    "    y=train_df[\"Classification\"],\n",
    ")\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Load configuration\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# Set dropout rates and number of labels\n",
    "config.attention_probs_dropout_prob = 0.2  # Increase dropout rate\n",
    "config.hidden_dropout_prob = 0.2  # Increase dropout rate\n",
    "config.num_labels = 7  # Number of output classes\n",
    "\n",
    "# Initialize the model with the modified configuration\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\", config=config\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the model\n",
    "save_path = r\"C:\\Users\\E.Sin\\Desktop\\Richmond\\DSA4264-Detoxify\\model-1\\distilbert\\model\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of training steps\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "# Define warmup steps (e.g., 10% of total steps)\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Scheduler with warmup for the first X steps, then linear decay\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of training steps\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "# Define warmup steps (e.g., 10% of total steps)\n",
    "num_warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "# Scheduler with warmup for the first X steps, then linear decay\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Training and Evaluation Functions with tqdm\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Wrap data_loader with tqdm to show progress\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Add scheduler step here\n",
    "\n",
    "        # Update progress bar description with running metrics\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"loss\": total_loss / (total_samples / BATCH_SIZE),\n",
    "                \"accuracy\": correct_predictions.double() / total_samples,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    accuracy = correct_predictions.double() / total_samples\n",
    "    return total_loss / len(data_loader), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [label for label, _ in sorted(label_mapping.items(), key=lambda x: x[1])]\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Wrap data_loader with tqdm to show progress during evaluation\n",
    "    progress_bar = tqdm(data_loader, desc=\"Validating\", unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "            # Update progress bar description with running metrics\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": total_loss / (total_samples / BATCH_SIZE),\n",
    "                    \"accuracy\": correct_predictions.double() / total_samples,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    accuracy = correct_predictions.double() / total_samples\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    # Print classification report to get precision, recall, and F1 per class\n",
    "    class_report = classification_report(\n",
    "        all_labels, all_preds, target_names=class_names\n",
    "    )\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "    return total_loss / len(data_loader), accuracy.item(), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 192/192 [07:27<00:00,  2.33s/batch, loss=1.29, accuracy=tensor(0.5247, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.2899, Train accuracy: 0.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 42/42 [00:09<00:00,  4.52batch/s, loss=0.832, accuracy=tensor(0.6846, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Hate/Toxic       0.46      0.71      0.56       750\n",
      "      Toxic 1       0.33      0.10      0.15       750\n",
      "      Toxic 2       0.46      0.55      0.50       750\n",
      "      Toxic 3       1.00      0.92      0.96       750\n",
      "       Hate 1       0.60      0.75      0.67       750\n",
      "       Hate 2       0.98      0.81      0.89       750\n",
      "       Hate 3       0.99      0.96      0.97       750\n",
      "\n",
      "     accuracy                           0.68      5250\n",
      "    macro avg       0.69      0.68      0.67      5250\n",
      " weighted avg       0.69      0.68      0.67      5250\n",
      "\n",
      "Validation loss: 0.8129, Validation accuracy: 0.6846, Validation F1: 0.6707\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 192/192 [05:58<00:00,  1.87s/batch, loss=0.776, accuracy=tensor(0.7021, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7741, Train accuracy: 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 42/42 [00:08<00:00,  4.82batch/s, loss=0.721, accuracy=tensor(0.7200, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Hate/Toxic       0.57      0.74      0.64       750\n",
      "      Toxic 1       0.38      0.36      0.37       750\n",
      "      Toxic 2       0.52      0.50      0.51       750\n",
      "      Toxic 3       1.00      0.92      0.96       750\n",
      "       Hate 1       0.68      0.76      0.72       750\n",
      "       Hate 2       0.99      0.81      0.89       750\n",
      "       Hate 3       0.99      0.96      0.97       750\n",
      "\n",
      "     accuracy                           0.72      5250\n",
      "    macro avg       0.73      0.72      0.72      5250\n",
      " weighted avg       0.73      0.72      0.72      5250\n",
      "\n",
      "Validation loss: 0.7042, Validation accuracy: 0.7200, Validation F1: 0.7236\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 192/192 [05:59<00:00,  1.87s/batch, loss=0.698, accuracy=tensor(0.7342, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6958, Train accuracy: 0.7342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 42/42 [00:08<00:00,  4.75batch/s, loss=0.701, accuracy=tensor(0.7267, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Hate/Toxic       0.63      0.70      0.66       750\n",
      "      Toxic 1       0.41      0.28      0.33       750\n",
      "      Toxic 2       0.49      0.63      0.55       750\n",
      "      Toxic 3       0.98      0.92      0.95       750\n",
      "       Hate 1       0.66      0.78      0.71       750\n",
      "       Hate 2       0.99      0.82      0.89       750\n",
      "       Hate 3       0.99      0.96      0.97       750\n",
      "\n",
      "     accuracy                           0.73      5250\n",
      "    macro avg       0.73      0.73      0.73      5250\n",
      " weighted avg       0.73      0.73      0.73      5250\n",
      "\n",
      "Validation loss: 0.6845, Validation accuracy: 0.7267, Validation F1: 0.7256\n",
      "Best model saved at C:\\Users\\E.Sin\\Desktop\\Richmond\\DSA4264-Detoxify\\model-1\\distilbert\\model\\best_distilbert_model.pth with validation loss: 0.6845\n"
     ]
    }
   ],
   "source": [
    "# Initialize variable to track the best validation loss\n",
    "best_val_loss = float(\"inf\")  # Start with a very high value\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # Pass the scheduler to train_epoch\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, device, scheduler\n",
    "    )\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    val_loss, val_acc, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "    print(\n",
    "        f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}, Validation F1: {val_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Check if the current validation loss is better than the best we've seen\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss  # Update the best validation loss\n",
    "        best_model = model.state_dict()  # Save the current model state\n",
    "\n",
    "# Save the best model after training\n",
    "model_save_path = os.path.join(save_path, \"best_distilbert_model.pth\")\n",
    "torch.save(best_model, model_save_path)\n",
    "print(\n",
    "    f\"Best model saved at {model_save_path} with validation loss: {best_val_loss:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact length of test data: 5250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 42/42 [00:09<00:00,  4.30batch/s, loss=0.744, accuracy=tensor(0.7120, device='cuda:0', dtype=torch.float64)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Hate/Toxic       0.61      0.69      0.65       750\n",
      "      Toxic 1       0.39      0.26      0.31       750\n",
      "      Toxic 2       0.46      0.59      0.51       750\n",
      "      Toxic 3       0.99      0.91      0.95       750\n",
      "       Hate 1       0.65      0.76      0.70       750\n",
      "       Hate 2       0.98      0.83      0.90       750\n",
      "       Hate 3       0.99      0.95      0.97       750\n",
      "\n",
      "     accuracy                           0.71      5250\n",
      "    macro avg       0.72      0.71      0.71      5250\n",
      " weighted avg       0.72      0.71      0.71      5250\n",
      "\n",
      "Test loss: 0.7264, Test accuracy: 0.7120, Test F1: 0.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Evaluate the Model on Test Set\n",
    "# Print the size of the test dataset\n",
    "test_data_size = len(test_loader.dataset)\n",
    "print(f\"Exact length of test data: {test_data_size}\\n\")  # Added new line for clarity\n",
    "\n",
    "# Evaluate the model on the test set and print metrics\n",
    "test_loss, test_acc, test_f1 = eval_model(model, test_loader, criterion, device)\n",
    "print(\n",
    "    f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
