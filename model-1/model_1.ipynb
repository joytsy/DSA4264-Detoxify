{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load on GPU\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tq\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the given CSV file into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(filepath, engine=\"python\")\n",
    "        print(f\"Data successfully loaded from {filepath}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reading the CSV file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from C:\\Users\\richm\\OneDrive\\Desktop\\DSA4264\\DSA4264-Detoxify\\clean_concatenated_texts_data_with_labels.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>link</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>moderation</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 82</th>\n",
       "      <th>Unnamed: 83</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>Unnamed: 85</th>\n",
       "      <th>Unnamed: 86</th>\n",
       "      <th>Unnamed: 87</th>\n",
       "      <th>Unnamed: 88</th>\n",
       "      <th>Unnamed: 89</th>\n",
       "      <th>Unnamed: 90</th>\n",
       "      <th>Unnamed: 91</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id of j2gwp5d said this: The best.</td>\n",
       "      <td>1/1/2023 4:51</td>\n",
       "      <td>MisoMesoMilo</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t1_j2gje0f</td>\n",
       "      <td>j2gwp5d</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id of j2i2mtw said this: Wah heart pain.\\nid o...</td>\n",
       "      <td>1/1/2023 14:03</td>\n",
       "      <td>MisoMesoMilo</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t1_j2hyfm2</td>\n",
       "      <td>j2i2mtw</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>id of j2h2alh said this: HNY DT where to go to...</td>\n",
       "      <td>1/1/2023 5:48</td>\n",
       "      <td>dazark</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>j2h2alh</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>id of j2hpea5 said this: Clubbing shenanigans,...</td>\n",
       "      <td>1/1/2023 11:08</td>\n",
       "      <td>N1_Procrastinator</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t1_j2ho0m4</td>\n",
       "      <td>j2hpea5</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id of j2hewei said this: Oooff...was supposed ...</td>\n",
       "      <td>1/1/2023 8:28</td>\n",
       "      <td>EaeleButEeelier</td>\n",
       "      <td>/r/singapore/comments/1004s1o/rsingapore_rando...</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>t3_1004s1o</td>\n",
       "      <td>j2hewei</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>1048555</td>\n",
       "      <td>id of fw5t8zt said this: Tharman will hard car...</td>\n",
       "      <td>6/27/2020 12:32</td>\n",
       "      <td>wank_for_peace</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw55rd0</td>\n",
       "      <td>fw5t8zt</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>1048556</td>\n",
       "      <td>id of fw5djn0 said this: Ivan whole day long p...</td>\n",
       "      <td>6/27/2020 8:16</td>\n",
       "      <td>show-up</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw52891</td>\n",
       "      <td>fw5djn0</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>1048557</td>\n",
       "      <td>id of fw5buyl said this: He is saying people a...</td>\n",
       "      <td>6/27/2020 7:47</td>\n",
       "      <td>CanISmellYourPanty</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw56nrv</td>\n",
       "      <td>fw5buyl</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>1048558</td>\n",
       "      <td>id of fw54fmr said this: Eh you want say can s...</td>\n",
       "      <td>6/27/2020 5:52</td>\n",
       "      <td>thrulim123</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw52891</td>\n",
       "      <td>fw54fmr</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>1048559</td>\n",
       "      <td>id of fw558gh said this: Dont need ah, he alre...</td>\n",
       "      <td>6/27/2020 6:03</td>\n",
       "      <td>freakshow504</td>\n",
       "      <td>/r/singapore/comments/hgnndv/ivan_responds/fw5...</td>\n",
       "      <td>t3_hgnndv</td>\n",
       "      <td>t1_fw53rc3</td>\n",
       "      <td>fw558gh</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'removal_reason': None, 'collapsed': False, '...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "0                0                 id of j2gwp5d said this: The best.   \n",
       "1                1  id of j2i2mtw said this: Wah heart pain.\\nid o...   \n",
       "2                2  id of j2h2alh said this: HNY DT where to go to...   \n",
       "3                3  id of j2hpea5 said this: Clubbing shenanigans,...   \n",
       "4                4  id of j2hewei said this: Oooff...was supposed ...   \n",
       "...            ...                                                ...   \n",
       "1048570    1048555  id of fw5t8zt said this: Tharman will hard car...   \n",
       "1048571    1048556  id of fw5djn0 said this: Ivan whole day long p...   \n",
       "1048572    1048557  id of fw5buyl said this: He is saying people a...   \n",
       "1048573    1048558  id of fw54fmr said this: Eh you want say can s...   \n",
       "1048574    1048559  id of fw558gh said this: Dont need ah, he alre...   \n",
       "\n",
       "               timestamp            username  \\\n",
       "0          1/1/2023 4:51        MisoMesoMilo   \n",
       "1         1/1/2023 14:03        MisoMesoMilo   \n",
       "2          1/1/2023 5:48              dazark   \n",
       "3         1/1/2023 11:08   N1_Procrastinator   \n",
       "4          1/1/2023 8:28     EaeleButEeelier   \n",
       "...                  ...                 ...   \n",
       "1048570  6/27/2020 12:32      wank_for_peace   \n",
       "1048571   6/27/2020 8:16             show-up   \n",
       "1048572   6/27/2020 7:47  CanISmellYourPanty   \n",
       "1048573   6/27/2020 5:52          thrulim123   \n",
       "1048574   6/27/2020 6:03        freakshow504   \n",
       "\n",
       "                                                      link     link_id  \\\n",
       "0        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "1        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "2        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "3        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "4        /r/singapore/comments/1004s1o/rsingapore_rando...  t3_1004s1o   \n",
       "...                                                    ...         ...   \n",
       "1048570  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048571  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048572  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048573  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "1048574  /r/singapore/comments/hgnndv/ivan_responds/fw5...   t3_hgnndv   \n",
       "\n",
       "          parent_id       id subreddit_id  \\\n",
       "0        t1_j2gje0f  j2gwp5d     t5_2qh8c   \n",
       "1        t1_j2hyfm2  j2i2mtw     t5_2qh8c   \n",
       "2        t3_1004s1o  j2h2alh     t5_2qh8c   \n",
       "3        t1_j2ho0m4  j2hpea5     t5_2qh8c   \n",
       "4        t3_1004s1o  j2hewei     t5_2qh8c   \n",
       "...             ...      ...          ...   \n",
       "1048570  t1_fw55rd0  fw5t8zt     t5_2qh8c   \n",
       "1048571  t1_fw52891  fw5djn0     t5_2qh8c   \n",
       "1048572  t1_fw56nrv  fw5buyl     t5_2qh8c   \n",
       "1048573  t1_fw52891  fw54fmr     t5_2qh8c   \n",
       "1048574  t1_fw53rc3  fw558gh     t5_2qh8c   \n",
       "\n",
       "                                                moderation  ... Unnamed: 82  \\\n",
       "0        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "1        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "2        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "3        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "4        {'controversiality': 0, 'collapsed_reason_code...  ...         NaN   \n",
       "...                                                    ...  ...         ...   \n",
       "1048570  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048571  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048572  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048573  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "1048574  {'removal_reason': None, 'collapsed': False, '...  ...         NaN   \n",
       "\n",
       "        Unnamed: 83 Unnamed: 84 Unnamed: 85 Unnamed: 86 Unnamed: 87  \\\n",
       "0               NaN         NaN         NaN         NaN         NaN   \n",
       "1               NaN         NaN         NaN         NaN         NaN   \n",
       "2               NaN         NaN         NaN         NaN         NaN   \n",
       "3               NaN         NaN         NaN         NaN         NaN   \n",
       "4               NaN         NaN         NaN         NaN         NaN   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1048570         NaN         NaN         NaN         NaN         NaN   \n",
       "1048571         NaN         NaN         NaN         NaN         NaN   \n",
       "1048572         NaN         NaN         NaN         NaN         NaN   \n",
       "1048573         NaN         NaN         NaN         NaN         NaN   \n",
       "1048574         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "        Unnamed: 88 Unnamed: 89 Unnamed: 90 Unnamed: 91  \n",
       "0               NaN         NaN         NaN         NaN  \n",
       "1               NaN         NaN         NaN         NaN  \n",
       "2               NaN         NaN         NaN         NaN  \n",
       "3               NaN         NaN         NaN         NaN  \n",
       "4               NaN         NaN         NaN         NaN  \n",
       "...             ...         ...         ...         ...  \n",
       "1048570         NaN         NaN         NaN         NaN  \n",
       "1048571         NaN         NaN         NaN         NaN  \n",
       "1048572         NaN         NaN         NaN         NaN  \n",
       "1048573         NaN         NaN         NaN         NaN  \n",
       "1048574         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[1048575 rows x 92 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_csv(\n",
    "    r\"C:\\Users\\richm\\OneDrive\\Desktop\\DSA4264\\DSA4264-Detoxify\\balanced_data.csv\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model training.\n",
    "MAX_LEN = 512\n",
    "num_classes = 7\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update class-to-id mapping for 7 classes\n",
    "cls_to_id = {\n",
    "    \"No Hate/Toxic\": 0,\n",
    "    \"Hate 1\": 1,\n",
    "    \"Hate 2\": 2,\n",
    "    \"Hate 3\": 3,\n",
    "    \"Toxic 1\": 4,\n",
    "    \"Toxic 2\": 5,\n",
    "    \"Toxic 3\": 6,\n",
    "}\n",
    "\n",
    "id_to_cls = {v: k for k, v in cls_to_id.items()}\n",
    "\n",
    "num_classes = 7  # Update the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richm\\AppData\\Local\\Temp\\ipykernel_23920\\1372939726.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['gold_label'] = filtered_df['gold_label'].map(cls_to_id)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integer format\n",
    "data[\"gold_label\"] = data[\"gold_label\"].map(cls_to_id)\n",
    "data = data.dropna(subset=[\"gold_label\"])\n",
    "data[\"gold_label\"] = data[\"gold_label\"].astype(int)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "df_train, df_valid = train_test_split(\n",
    "    data, test_size=0.2, shuffle=True, stratify=data[\"gold_label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.targets = df[\"gold_label\"].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        text = \" \".join(text.split())  # Clean text\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
    "            \"token_type_ids\": inputs[\"token_type_ids\"].flatten(),\n",
    "            \"targets\": torch.tensor(self.targets[index], dtype=torch.long),\n",
    "            \"text\": text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = CustomDataset(df_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(df_valid, tokenizer, MAX_LEN)\n",
    "\n",
    "# Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Increase num_workers for faster loading\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", return_dict=True\n",
    "        )\n",
    "\n",
    "        # Additional fully connected and dropout layers\n",
    "        self.fc1 = torch.nn.Linear(768, 512)\n",
    "        self.dropout1 = torch.nn.Dropout(0.3)\n",
    "        self.fc2 = torch.nn.Linear(512, 256)\n",
    "        self.dropout2 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        # Final output layer for classification\n",
    "        self.linear = torch.nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        x = self.fc1(output.pooler_output)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights based on the balanced data\n",
    "class_counts = data[\"gold_label\"].value_counts().sort_index()\n",
    "class_weights = 1.0 / class_counts  # Inverse of class frequency\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize the weights\n",
    "\n",
    "# Convert to a tensor and move to the correct device (GPU)\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float).to(device)\n",
    "\n",
    "# Use class weights in CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/900 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = BERTClass(num_classes=num_classes)\n",
    "\n",
    "# # Wrap the model with DataParallel to use multiple GPUs\n",
    "# model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Move the model to the appropriate device (use all available GPUs)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function (already set with class weights) and optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()  # Record the start time of the epoch\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Create a progress bar for the training data loader\n",
    "    train_loader_tqdm = tqdm(\n",
    "        train_data_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Training step with progress bar\n",
    "    for batch in train_loader_tqdm:\n",
    "        ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        labels = batch[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(ids, mask, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Update the progress bar with loss\n",
    "        train_loader_tqdm.set_postfix(\n",
    "            {\"train_loss\": total_loss / (train_loader_tqdm.n + 1)}\n",
    "        )\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    accuracy = correct_predictions / len(train_data_loader.dataset)\n",
    "\n",
    "    # Record the end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time  # Time taken for this epoch\n",
    "\n",
    "    print(f\"Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    # Validation step with progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    # Create a progress bar for the validation data loader\n",
    "    val_loader_tqdm = tqdm(val_data_loader, desc=\"Validating\", unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_tqdm:\n",
    "            ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            labels = batch[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "            logits = model(ids, mask, token_type_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            val_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            # Update the progress bar with validation loss\n",
    "            val_loader_tqdm.set_postfix(\n",
    "                {\"val_loss\": val_loss / (val_loader_tqdm.n + 1)}\n",
    "            )\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_data_loader)\n",
    "    val_accuracy = val_correct / len(val_data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "    )\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model weights\n",
    "model_save_path = \"./model1/bert_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = BERTClass(num_classes=num_classes)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a validation DataLoader\n",
    "\n",
    "\n",
    "def get_predictions_and_labels(model, data_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    texts = []\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"targets\"].to(device)\n",
    "            text_batch = batch[\"text\"]  # Collecting texts\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            texts.extend(text_batch)  # Collect texts\n",
    "            probabilities.extend(probs.cpu().numpy())  # Store probabilities\n",
    "\n",
    "    return np.array(predictions), np.array(true_labels), texts, np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and labels for validation set\n",
    "predictions, true_labels, texts, probabilities = get_predictions_and_labels(\n",
    "    model, val_data_loader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class-wise accuracy\n",
    "\n",
    "\n",
    "def calculate_class_wise_accuracy(conf_matrix):\n",
    "    # True Positives for each class are the diagonal elements\n",
    "    true_positives = np.diag(conf_matrix)\n",
    "\n",
    "    # Support (Total actual instances for each class)\n",
    "    support = conf_matrix.sum(axis=1)\n",
    "\n",
    "    # Class-wise accuracy\n",
    "    class_wise_accuracy = true_positives / support\n",
    "\n",
    "    return class_wise_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get misclassified samples\n",
    "\n",
    "\n",
    "def get_misclassified_samples(predictions, true_labels, texts):\n",
    "    misclassified = []\n",
    "\n",
    "    for pred, true, text in zip(predictions, true_labels, texts):\n",
    "        if pred != true:\n",
    "            misclassified.append(\n",
    "                {\"text\": text, \"true_label\": true, \"predicted_label\": pred}\n",
    "            )\n",
    "\n",
    "    return misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "class_wise_accuracy = calculate_class_wise_accuracy(conf_matrix)\n",
    "\n",
    "# Print class-wise accuracy\n",
    "for i, accuracy in enumerate(class_wise_accuracy):\n",
    "    print(f\"Class {id_to_cls[i]} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=list(cls_to_id.keys()),\n",
    "    yticklabels=list(cls_to_id.keys()),\n",
    ")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified samples\n",
    "misclassified_samples = get_misclassified_samples(predictions, true_labels, texts)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "misclassified_df = pd.DataFrame(misclassified_samples)\n",
    "\n",
    "misclassified_df[\"true_label\"] = misclassified_df[\"true_label\"].apply(\n",
    "    lambda x: id_to_cls[x]\n",
    ")\n",
    "misclassified_df[\"predicted_label\"] = misclassified_df[\"predicted_label\"].apply(\n",
    "    lambda x: id_to_cls[x]\n",
    ")\n",
    "\n",
    "# Display misclassified samples\n",
    "print(misclassified_df.shape, len(predictions))\n",
    "# misclassified_df.to_csv('./data/misclassified_df.csv', index=False)\n",
    "misclassified_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
